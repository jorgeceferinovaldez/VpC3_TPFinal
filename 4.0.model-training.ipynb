{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8785215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n",
    "from timm.models import create_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from scipy import ndimage\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018dbbe",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a924e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMultiClassClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Clasificador Multi-Clase Vision Transformer para Detección de Defectos en Cuero y Detección de Anomalías.\n",
    "    Esta clase implementa un enfoque híbrido que combina clasificación multi-clase y detección de anomalías\n",
    "    usando un backbone Vision Transformer (ViT). Está específicamente diseñado para detección de defectos en cuero\n",
    "    con 6 clases incluyendo muestras tanto defectuosas como no defectuosas.\n",
    "    \n",
    "    El modelo soporta:\n",
    "    - Clasificación multi-clase de defectos en cuero\n",
    "    - Detección de anomalías usando puntuación híbrida (clasificación + similitud)\n",
    "    - Generación de mapas de atención para explicabilidad\n",
    "    - Extracción y almacenamiento de características para muestras normales\n",
    "    \n",
    "    Atributos:\n",
    "        backbone (timm.models): Modelo ViT-Base/16 usado como extractor de características\n",
    "        classifier (nn.Sequential): Cabezal de clasificación personalizado para 6 clases\n",
    "        normal_features (torch.Tensor): Características almacenadas de muestras normales (no defectuosas)\n",
    "        class_names (list): Nombres de las 6 clases de defectos en cuero\n",
    "    \n",
    "    Clases:\n",
    "        0: folding_marks - Arrugas o pliegues en la superficie del cuero\n",
    "        1: grain_off - Patrón de grano irregular\n",
    "        2: growth_marks - Marcas de crecimiento natural en el cuero\n",
    "        3: loose_grain - Capa de grano suelta o desprendida\n",
    "        4: pinhole - Pequeños agujeros en la superficie del cuero\n",
    "        5: non_defective - Cuero normal sin defectos\n",
    "    \n",
    "    Ejemplo:\n",
    "        >>> model = ViTMultiClassClassifier(num_classes=6, pretrained=True)\n",
    "        >>> model.to(device)\n",
    "        >>> \n",
    "        >>> # Clasificación multi-clase\n",
    "        >>> results = model.classify_multiclass(images)\n",
    "        >>> predicted_classes = results['predicted_classes']\n",
    "        >>> \n",
    "        >>> # Almacenar características normales para detección de anomalías\n",
    "        >>> model.store_normal_features(train_dataloader, device)\n",
    "        >>> \n",
    "        >>> # Detección híbrida de anomalías\n",
    "        >>> anomaly_results = model.detect_anomaly_hybrid(test_images)\n",
    "        >>> anomaly_scores = anomaly_results['anomaly_scores']\n",
    "        >>> \n",
    "        >>> # Generar mapas de atención para explicabilidad\n",
    "        >>> attention_maps = model.extract_attention_maps(images)\n",
    "    \n",
    "    Notas:\n",
    "        - El modelo espera imágenes de entrada de tamaño (224, 224, 3)\n",
    "        - Se asume que la clase normal es la clase 4 (non_defective)\n",
    "        - La detección de anomalías requiere llamar primero store_normal_features()\n",
    "        - Los mapas de atención se generan usando análisis de características visuales (bordes, textura, esquinas)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=6, pretrained=True):\n",
    "        super(ViTMultiClassClassifier, self).__init__()\n",
    "\n",
    "        # ViT-Base/16 como feature extractor\n",
    "        self.backbone = create_model(\n",
    "            'vit_base_patch16_224',\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0 # Sin capa de clasificación\n",
    "        )\n",
    "\n",
    "        # Head de clasificación personalizado para 6 clases\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # Para almacenar features de clases normales (detección de anomalías)\n",
    "        self.normal_features = None\n",
    "        self.class_names = [\n",
    "            \"folding_marks\",\n",
    "            \"grain_off\", \n",
    "            \"growth_marks\",\n",
    "            \"loose_grain\",\n",
    "            \"pinhole\",\n",
    "            \"non_defective\",\n",
    "        ]\n",
    "\n",
    "    def _generate_realistic_attention(self, images_tensor):\n",
    "        \"\"\"\n",
    "        Genera mapas de atención realistas para un lote de imágenes.\n",
    "        Este método procesa un lote de imágenes para crear mapas de atención que resaltan\n",
    "        regiones visualmente importantes. Realiza normalización de imágenes, convierte a escala de grises,\n",
    "        y calcula características de atención visual para cada imagen en el lote.\n",
    "        Args:\n",
    "            images_tensor (torch.Tensor): Tensor de entrada que contiene un lote de imágenes.\n",
    "                        Forma esperada: [batch_size, 3, 224, 224]\n",
    "                        Las imágenes deben estar normalizadas con estadísticas de ImageNet.\n",
    "        Returns:\n",
    "            list: Una lista de arrays de numpy, cada uno representando un mapa de atención de forma (14, 14).\n",
    "              Cada mapa de atención contiene valores que indican la importancia de diferentes\n",
    "              regiones espaciales en la imagen de entrada correspondiente. Devuelve mapas de atención\n",
    "              de respaldo (valores uniformes de 0.5) en caso de errores o entradas inválidas.\n",
    "        Notes:\n",
    "            - Se realiza validación de entrada para asegurar formato de tensor y dimensiones correctas\n",
    "            - Las imágenes se desnormalizan usando estadísticas de ImageNet para análisis visual\n",
    "            - La conversión a escala de grises usa pesos estándar RGB a escala de grises (0.299, 0.587, 0.114)\n",
    "            - El manejo de errores asegura respaldo elegante para imágenes problemáticas\n",
    "            - Se imprime información de depuración durante el procesamiento\n",
    "        Raises:\n",
    "            No se lanzan excepciones directamente, pero los errores se capturan y registran con\n",
    "            mapas de atención de respaldo devueltos para casos fallidos.\n",
    "        \"\"\"\n",
    "        # Validar input\n",
    "        if not isinstance(images_tensor, torch.Tensor):\n",
    "            print(f\" Input no es tensor: {type(images_tensor)}\")\n",
    "            return [np.ones((14, 14)) * 0.5] * 1\n",
    "            \n",
    "        batch_size = images_tensor.shape[0]\n",
    "        device = images_tensor.device\n",
    "        \n",
    "        print(f\" Procesando batch de {batch_size} imágenes...\")\n",
    "        \n",
    "        attention_maps = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                # Obtener imagen individual\n",
    "                img = images_tensor[i]  # [3, 224, 224]\n",
    "                \n",
    "                # Validar dimensiones\n",
    "                if len(img.shape) != 3 or img.shape[0] != 3:\n",
    "                    print(f\" Imagen {i} tiene dimensiones incorrectas: {img.shape}\")\n",
    "                    attention_maps.append(np.ones((14, 14)) * 0.5)\n",
    "                    continue\n",
    "                \n",
    "                # Convertir a numpy para análisis\n",
    "                img_np = img.detach().cpu().numpy()\n",
    "                img_np = np.transpose(img_np, (1, 2, 0))  # [224, 224, 3]\n",
    "                \n",
    "                # Desnormalizar para análisis visual\n",
    "                img_display = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "                img_display = np.clip(img_display, 0, 1)\n",
    "                \n",
    "                # Convertir a escala de grises\n",
    "                gray = np.dot(img_display, [0.299, 0.587, 0.114])\n",
    "                \n",
    "                # Análisis de características visuales\n",
    "                attention_map = self._compute_visual_attention(gray, i)\n",
    "                attention_maps.append(attention_map)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Error procesando imagen {i}: {e}\")\n",
    "                # Fallback seguro\n",
    "                attention_maps.append(np.ones((14, 14)) * 0.5)\n",
    "        \n",
    "        return attention_maps\n",
    "\n",
    "    def _compute_visual_attention(self, gray_image, image_index):\n",
    "        \"\"\"\n",
    "        Calcula un mapa de atención visual para una imagen en escala de grises usando múltiples técnicas de detección de características.\n",
    "        Este método genera un mapa de atención combinando detección de bordes, variación de textura,\n",
    "        detección de esquinas y un componente aleatorio determinístico basado en el contenido de la imagen.\n",
    "        El mapa de atención final se normaliza, suaviza y redimensiona a 14x14 píxeles.\n",
    "        \n",
    "        Args:\n",
    "            gray_image (numpy.ndarray): Imagen de entrada en escala de grises como array 2D de numpy.\n",
    "            image_index (int): Índice de la imagen (actualmente no se usa en el cálculo).\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Un mapa de atención normalizado de 14x14 con valores entre 0 y 1,\n",
    "                  donde valores más altos indican regiones de mayor importancia visual.\n",
    "                  Devuelve un mapa de atención uniforme de 0.5 si el cálculo falla.\n",
    "                  \n",
    "        Proceso:\n",
    "            1. Detección de bordes usando operadores Sobel en direcciones X e Y\n",
    "            2. Análisis de variación de textura usando filtrado de varianza local\n",
    "            3. Detección de esquinas/puntos de interés usando desviación estándar local\n",
    "            4. Combinación ponderada de características (40% bordes, 30% textura, 30% esquinas)\n",
    "            5. Adición de componente aleatorio determinístico basado en contenido de imagen\n",
    "            6. Normalización al rango [0, 1]\n",
    "            7. Suavizado gaussiano con sigma=1.0\n",
    "            8. Redimensionar a 14x14 usando interpolación de área\n",
    "            \n",
    "        Raises:\n",
    "            Exception: Captura y registra cualquier error de cálculo, devolviendo mapa de atención por defecto.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Detección de bordes\n",
    "            sobel_x = ndimage.sobel(gray_image, axis=1)\n",
    "            sobel_y = ndimage.sobel(gray_image, axis=0)\n",
    "            edges = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "            \n",
    "            # 2. Variación de textura\n",
    "            texture_var = ndimage.generic_filter(gray_image, np.var, size=5)\n",
    "            \n",
    "            # 3. Detección de esquinas/puntos de interés\n",
    "            corners = ndimage.generic_filter(gray_image, np.std, size=3)\n",
    "            \n",
    "            # 4. Combinar características\n",
    "            combined_attention = (\n",
    "                0.4 * edges + \n",
    "                0.3 * texture_var + \n",
    "                0.3 * corners\n",
    "            )\n",
    "            \n",
    "            # 5. Agregar variación basada en contenido de imagen\n",
    "            seed_value = int(np.sum(gray_image * 1000) % 10000)\n",
    "            np.random.seed(seed_value)\n",
    "            random_component = np.random.rand(*gray_image.shape) * 0.2\n",
    "            \n",
    "            # 6. Combinar todo\n",
    "            final_attention = combined_attention + random_component\n",
    "            \n",
    "            # 7. Normalizar\n",
    "            if final_attention.max() > final_attention.min():\n",
    "                final_attention = (final_attention - final_attention.min()) / \\\n",
    "                                (final_attention.max() - final_attention.min())\n",
    "            else:\n",
    "                final_attention = np.ones_like(final_attention) * 0.5\n",
    "            \n",
    "            # 8. Suavizar\n",
    "            final_attention = ndimage.gaussian_filter(final_attention, sigma=1.0)\n",
    "            \n",
    "            # 9. Redimensionar a 14x14\n",
    "            attention_14x14 = cv2.resize(final_attention, (14, 14), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            return attention_14x14\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error en compute_visual_attention: {e}\")\n",
    "            return np.ones((14, 14)) * 0.5\n",
    "\n",
    "    def extract_attention_maps(self, x, use_cls_attention=True):\n",
    "        \"\"\"\n",
    "        Extrae mapas de atención del tensor de entrada para visualización y análisis.\n",
    "        Este método genera mapas de atención que resaltan regiones de interés en las imágenes de entrada.\n",
    "        Valida el formato del tensor de entrada y genera patrones de atención realistas para detección de anomalías.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor de entrada con forma [B, C, H, W] donde:\n",
    "            - B: tamaño del lote\n",
    "            - C: número de canales\n",
    "            - H: altura\n",
    "            - W: ancho\n",
    "            use_cls_attention (bool, opcional): Si usar atención del token de clase. \n",
    "            Por defecto True.\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de mapas de atención como arrays de numpy. Cada mapa de atención tiene forma (14, 14).\n",
    "            Devuelve mapas de atención por defecto (valores 0.5) si falla la validación de entrada.\n",
    "            \n",
    "        Raises:\n",
    "            No se lanzan excepciones. Entradas inválidas devuelven mapas de atención por defecto con \n",
    "            mensajes de advertencia apropiados impresos en consola.\n",
    "            \n",
    "        Note:\n",
    "            - Establece el modelo en modo de evaluación durante el procesamiento\n",
    "            - Usa contexto torch.no_grad() para eficiencia de memoria\n",
    "            - Imprime estadísticas de validación para cada mapa de atención generado\n",
    "            - La validación de entrada asegura tipo tensor y requisitos de forma 4D\n",
    "        \"\"\"\n",
    "        # Validar input\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            print(f\" Input debe ser tensor, recibido: {type(x)}\")\n",
    "            return [np.ones((14, 14)) * 0.5]\n",
    "            \n",
    "        if len(x.shape) != 4:\n",
    "            print(f\" Input debe tener 4 dimensiones [B,C,H,W], recibido: {x.shape}\")\n",
    "            return [np.ones((14, 14)) * 0.5] * x.shape[0]\n",
    "        \n",
    "        self.eval()\n",
    "        print(f\" Generando attention maps para {x.size(0)} imágenes...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            attention_maps = self._generate_realistic_attention(x)\n",
    "            \n",
    "            print(f\" Generados {len(attention_maps)} attention maps\")\n",
    "            \n",
    "            # Verificar que todos los mapas son válidos\n",
    "            for i, attn_map in enumerate(attention_maps):\n",
    "                if attn_map is not None and hasattr(attn_map, 'shape'):\n",
    "                    min_val, max_val = attn_map.min(), attn_map.max()\n",
    "                    std_val = attn_map.std()\n",
    "                    print(f\"  Mapa {i}: min={min_val:.3f}, max={max_val:.3f}, std={std_val:.3f}\")\n",
    "                else:\n",
    "                    print(f\"   Mapa {i} es inválido\")\n",
    "        \n",
    "        return attention_maps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass completo: features + clasificación\"\"\"\n",
    "        features = self.backbone(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits, features\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Solo extracción de features sin clasificación\"\"\"\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def store_normal_features(self, dataloader, device):\n",
    "        \"\"\"\n",
    "        Extraer y almacenar representaciones de características de imágenes normales (sin defectos).\n",
    "        Este método procesa un dataloader para extraer características de imágenes etiquetadas como\n",
    "        'non_defective' (clase 4) y las almacena para uso posterior en detección de anomalías.\n",
    "        El modelo se establece en modo de evaluación durante la extracción de características.\n",
    "        \n",
    "        Args:\n",
    "            dataloader (torch.utils.data.DataLoader): DataLoader que contiene imágenes y etiquetas\n",
    "            device (torch.device): Dispositivo para ejecutar el cálculo (CPU o GPU)\n",
    "            \n",
    "        Returns:\n",
    "            None: Almacena las características extraídas en el atributo self.normal_features\n",
    "            \n",
    "        Efectos secundarios:\n",
    "            - Establece el modelo en modo de evaluación\n",
    "            - Llena self.normal_features con tensores de características concatenados\n",
    "            - Imprime el progreso de extracción y estadísticas\n",
    "            - Crea características dummy si no se encuentran imágenes normales\n",
    "            \n",
    "        Nota:\n",
    "            - Solo procesa imágenes con etiqueta == 4 (clase non_defective en el dataset de Kaggle)\n",
    "            - Las características se mueven a CPU para almacenamiento\n",
    "            - Imprime estadísticas de características incluyendo desviación estándar y norma\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        normal_features = []\n",
    "\n",
    "        print(\"Extrayendo features de imágenes normales...\")\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(dataloader, desc=\"Procesando features normales\"):\n",
    "                images = images.to(device)\n",
    "                features = self.extract_features(images)\n",
    "\n",
    "                for i, label in enumerate(labels):\n",
    "                    if label.item() == 4:  # non_defective\n",
    "                        normal_features.append(features[i:i+1].cpu())\n",
    "\n",
    "        if normal_features:\n",
    "            self.normal_features = torch.cat(normal_features, dim=0)\n",
    "            print(f\" Almacenadas {len(self.normal_features)} features normales\")\n",
    "        else:\n",
    "            print(\" No se encontraron imágenes normales\")\n",
    "            self.normal_features = torch.randn(10, 768)\n",
    "\n",
    "    def classify_multiclass(self, x):\n",
    "        \"\"\"Clasificación multi-clase estándar\"\"\"\n",
    "        logits, features = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        predicted_classes = torch.argmax(probs, dim=1)\n",
    "\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'probabilities': probs,\n",
    "            'predicted_classes': predicted_classes,\n",
    "            'features': features\n",
    "        }\n",
    "\n",
    "    def detect_anomaly_hybrid(self, x):\n",
    "        \"\"\"\n",
    "        Detectar anomalías usando un enfoque híbrido que combina métodos de clasificación y similitud.\n",
    "        Este método implementa una estrategia de detección de anomalías multifacética que combina:\n",
    "        1. Puntuación basada en clasificación usando probabilidad de clase normal\n",
    "        2. Puntuación de similitud coseno contra características normales conocidas\n",
    "        3. Combinación híbrida ponderada de ambos enfoques\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor de entrada de forma (batch_size, channels, height, width)\n",
    "                     que contiene las imágenes a analizar para detectar anomalías.\n",
    "        Returns:\n",
    "            dict: Un diccionario que contiene las siguientes claves:\n",
    "            - 'anomaly_scores' (torch.Tensor): Puntuaciones de anomalía híbridas (0-1, mayor = más anómalo)\n",
    "            - 'similarity_scores' (torch.Tensor): Puntuaciones de anomalía basadas en similitud coseno\n",
    "            - 'classification_scores' (torch.Tensor): Puntuaciones de anomalía basadas en clasificación\n",
    "            - 'predicted_classes' (torch.Tensor): Índices de clases predichas\n",
    "            - 'class_probabilities' (torch.Tensor): Probabilidades softmax para todas las clases\n",
    "            - 'features' (torch.Tensor): Representaciones de características extraídas\n",
    "            - 'normal_class_prob' (torch.Tensor): Probabilidad de clase normal (clase 4)\n",
    "        Notas:\n",
    "            - Usa peso alpha=0.7 para puntuación de similitud y beta=0.3 para puntuación de clasificación\n",
    "            - Asume que la clase 4 representa la clase normal 'non_defective'\n",
    "            - Si normal_features es None, recurre a puntuación solo de clasificación\n",
    "            - Todas las puntuaciones de anomalía están normalizadas al rango [0, 1] donde 1 indica alta probabilidad de anomalía\n",
    "        \"\"\"\n",
    "        logits, features = self.forward(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        predicted_classes = torch.argmax(probs, dim=1)\n",
    "\n",
    "        # Método 1: Probabilidad de clase normal\n",
    "        normal_class_prob = probs[:, 4]\n",
    "        classification_anomaly_score = 1.0 - normal_class_prob\n",
    "\n",
    "        # Método 2: Similitud coseno\n",
    "        if self.normal_features is not None:\n",
    "            features_norm = F.normalize(features, p=2, dim=1)\n",
    "            normal_features_norm = F.normalize(self.normal_features.to(features.device), p=2, dim=1)\n",
    "            similarities = torch.mm(features_norm, normal_features_norm.T)\n",
    "            max_similarities, _ = torch.max(similarities, dim=1)\n",
    "            similarity_anomaly_score = 1.0 - max_similarities\n",
    "        else:\n",
    "            similarity_anomaly_score = classification_anomaly_score\n",
    "\n",
    "        # Método 3: Combinación híbrida (como sugiere el paper)\n",
    "        # Combinar clasificación y similitud con pesos\n",
    "        alpha = 0.7 # peso para similitud coseno\n",
    "        beta = 0.3 # peso para clasificación\n",
    "        hybrid_anomaly_score = (alpha * similarity_anomaly_score + \n",
    "                               beta * classification_anomaly_score) # Normalizar al rango [0, 1]\n",
    "\n",
    "        return {\n",
    "            'anomaly_scores': hybrid_anomaly_score,\n",
    "            'similarity_scores': similarity_anomaly_score,\n",
    "            'classification_scores': classification_anomaly_score,\n",
    "            'predicted_classes': predicted_classes,\n",
    "            'class_probabilities': probs,\n",
    "            'features': features,\n",
    "            'normal_class_prob': normal_class_prob\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e5b73",
   "metadata": {},
   "source": [
    "### Funciones creadoras de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f203bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeatherDefectDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Una clase Dataset de PyTorch para cargar y preprocesar imágenes de defectos en cuero.\n",
    "    Este dataset carga imágenes de un dataset de defectos en cuero de Kaggle con 6 clases:\n",
    "    folding_marks, grain_off, growth_marks, loose_grains, non_defective, y pinhole.\n",
    "    Automáticamente divide los datos en conjuntos de entrenamiento y validación manteniendo\n",
    "    la distribución de clases.\n",
    "    \n",
    "    Args:\n",
    "        root_path (str): Ruta al directorio raíz que contiene las carpetas de clases\n",
    "        is_train (bool, opcional): Si es True, carga el conjunto de entrenamiento; si es False, \n",
    "            carga el conjunto de validación. Por defecto True.\n",
    "        validation_split (float, opcional): Fracción de datos a usar para validación (0-1). \n",
    "            Por defecto 0.2.\n",
    "        transform (callable, opcional): Transformación opcional a aplicar a las imágenes. \n",
    "            Por defecto None.\n",
    "        random_seed (int, opcional): Semilla aleatoria para divisiones reproducibles de \n",
    "            entrenamiento/validación. Por defecto 42.\n",
    "    \n",
    "    Atributos:\n",
    "        folder_to_class (dict): Mapeo de nombres de carpetas a índices de clases\n",
    "        class_names (list): Lista de nombres de clases en orden de índices de clases\n",
    "        image_paths (list): Lista de rutas a todas las imágenes en la división actual\n",
    "        labels (list): Lista de etiquetas de clase correspondientes para cada imagen\n",
    "    \n",
    "    Métodos:\n",
    "        _load_data(): Método interno para cargar y dividir el dataset\n",
    "        __len__(): Devuelve el número total de muestras en la división actual\n",
    "        __getitem__(idx): Devuelve una tupla de (imagen, etiqueta) para el índice dado\n",
    "    \n",
    "    Nota:\n",
    "        El dataset espera la siguiente estructura de carpetas:\n",
    "        root_path/\n",
    "        ├── folding_marks/\n",
    "        ├── grain_off/\n",
    "        ├── growth_marks/\n",
    "        ├── loose_grains/\n",
    "        ├── non_defective/\n",
    "        └── pinhole/\n",
    "    \"\"\"\n",
    "    def __init__(self, root_path, is_train=True, validation_split=0.2, transform=None, random_seed=42):\n",
    "        self.root_path = root_path\n",
    "        self.is_train = is_train\n",
    "        self.validation_split = validation_split\n",
    "        self.transform = transform\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        # Mapeo exacto de las carpetas del dataset de Kaggle a clases\n",
    "        self.folder_to_class = {\n",
    "            'folding_marks': 0,      # folding_marks\n",
    "            'grain_off': 1,          # grain_off  \n",
    "            'growth_marks': 2,       # growth_marks\n",
    "            'loose_grains': 3,       # loose_grain (nota: 'grains' en plural en Kaggle)\n",
    "            'non_defective': 4,      # non_defective\n",
    "            'pinhole': 5             # pinhole\n",
    "        }\n",
    "        \n",
    "        # Nombres de clases para el modelo (mantenemos consistencia con el paper)\n",
    "        self.class_names = [\n",
    "            'folding_marks',    # 0\n",
    "            'grain_off',        # 1  \n",
    "            'growth_marks',     # 2\n",
    "            'loose_grain',      # 3 (singular como en el paper)\n",
    "            'non_defective',    # 4\n",
    "            'pinhole'           # 5\n",
    "        ]\n",
    "        \n",
    "        self._load_data()\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Cargar y dividir el dataset de imágenes en conjuntos de entrenamiento y validación.\n",
    "        Este método carga imágenes desde el directorio raíz especificado, organizándolas por \n",
    "        carpetas de clases y dividiéndolas en conjuntos de entrenamiento y validación basándose \n",
    "        en la proporción de división de validación configurada. La división se realiza de manera \n",
    "        consistente usando una semilla aleatoria.\n",
    "        \n",
    "        El método puebla los siguientes atributos de instancia:\n",
    "        - self.image_paths: Lista de rutas de archivo a las imágenes seleccionadas\n",
    "        - self.labels: Lista de IDs de clase correspondientes para cada imagen\n",
    "        \n",
    "        Estructura de Directorio Esperada:\n",
    "            root_path/\n",
    "            ├── carpeta_clase_1/\n",
    "            │   ├── imagen1.jpg\n",
    "            │   └── imagen2.png\n",
    "            └── carpeta_clase_2/\n",
    "                ├── imagen3.jpeg\n",
    "                └── imagen4.jpg\n",
    "        \n",
    "        Proceso:\n",
    "        1. Escanea cada carpeta de clase definida en self.folder_to_class\n",
    "        2. Recopila todos los archivos de imagen válidos (.png, .jpg, .jpeg)\n",
    "        3. Mezcla aleatoriamente las imágenes dentro de cada clase usando self.random_seed\n",
    "        4. Divide cada clase según la proporción self.validation_split\n",
    "        5. Selecciona el subconjunto de entrenamiento o validación basado en la bandera self.is_train\n",
    "        6. Imprime estadísticas detalladas sobre el proceso de carga y división\n",
    "        \n",
    "        Excepciones:\n",
    "            Maneja implícitamente directorios faltantes imprimiendo advertencias y continuando\n",
    "            con listas de imágenes vacías para esas clases.\n",
    "        \n",
    "        Nota:\n",
    "            La división de validación se aplica por clase para mantener la distribución de clases\n",
    "            tanto en los conjuntos de entrenamiento como de validación.\n",
    "        \"\"\"\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        print(f\"Cargando desde: {self.root_path}\")\n",
    "        print(f\"Carpetas esperadas: {list(self.folder_to_class.keys())}\")\n",
    "        \n",
    "        # Recopilar todas las imágenes por clase\n",
    "        all_images_by_class = {}\n",
    "        \n",
    "        for folder_name, class_id in self.folder_to_class.items():\n",
    "            class_dir = os.path.join(self.root_path, folder_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                images = [os.path.join(class_dir, f) for f in os.listdir(class_dir) \n",
    "                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                all_images_by_class[class_id] = images\n",
    "                print(f\"   {folder_name}: {len(images)} imágenes → clase {class_id} ({self.class_names[class_id]})\")\n",
    "            else:\n",
    "                print(f\"   No encontrado: {class_dir}\")\n",
    "                all_images_by_class[class_id] = []\n",
    "        \n",
    "        # Dividir cada clase en train/validation\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        for class_id, images in all_images_by_class.items():\n",
    "            if len(images) > 0:\n",
    "                # Mezclar imágenes\n",
    "                images = np.array(images)\n",
    "                indices = np.random.permutation(len(images))\n",
    "                images = images[indices]\n",
    "                \n",
    "                # Dividir en train/validation\n",
    "                n_val = int(len(images) * self.validation_split)\n",
    "                \n",
    "                if self.is_train:\n",
    "                    # Usar para entrenamiento (80%)\n",
    "                    selected_images = images[n_val:]\n",
    "                else:\n",
    "                    # Usar para validación (20%)\n",
    "                    selected_images = images[:n_val]\n",
    "                \n",
    "                self.image_paths.extend(selected_images.tolist())\n",
    "                self.labels.extend([class_id] * len(selected_images))\n",
    "        \n",
    "        print(f\"\\n DIVISIÓN TRAIN/VALIDATION:\")\n",
    "        print(f\"Modo: {'Entrenamiento' if self.is_train else 'Validación'}\")\n",
    "        print(f\"Total imágenes: {len(self.image_paths)}\")\n",
    "        \n",
    "        # Mostrar distribución por clase\n",
    "        unique_labels, counts = np.unique(self.labels, return_counts=True)\n",
    "        for class_id, count in zip(unique_labels, counts):\n",
    "            print(f\"  {self.class_names[class_id]}: {count} imágenes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Recuperar una imagen y su etiqueta correspondiente en el índice especificado.\n",
    "        Args:\n",
    "            idx (int): Índice del elemento a recuperar del dataset.\n",
    "        Returns:\n",
    "            tuple: Una tupla que contiene:\n",
    "                - image (torch.Tensor o PIL.Image): Los datos de imagen procesados. Si se aplica\n",
    "                  transform, devuelve un tensor; de lo contrario devuelve una imagen PIL en formato RGB.\n",
    "                - label: La etiqueta correspondiente para la imagen en el índice dado.\n",
    "        Nota:\n",
    "            - Las imágenes se convierten automáticamente a formato RGB al cargarlas.\n",
    "            - Si se especifica una transformación durante la inicialización del dataset, se\n",
    "              aplicará a la imagen antes de devolverla.\n",
    "        \"\"\"\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf7a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTecTestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Clase Dataset para cargar datos de prueba de MVTec Anomaly Detection.\n",
    "    Este dataset está específicamente diseñado para tareas de clasificación binaria (normal vs anomalía)\n",
    "    usando la división de prueba del dataset MVTec AD. Carga imágenes de una categoría especificada\n",
    "    y asigna etiquetas binarias donde las muestras 'good' son etiquetadas como normales (0) y todos\n",
    "    los tipos de defectos son etiquetados como anomalías (1).\n",
    "    Args:\n",
    "        root_path (str): Ruta del directorio raíz que contiene el dataset MVTec AD\n",
    "        category (str, opcional): Categoría de producto a cargar (ej. 'leather', 'bottle'). \n",
    "                                 Por defecto 'leather'\n",
    "        transform (callable, opcional): Transformación opcional a aplicar a las imágenes.\n",
    "                                       Por defecto None\n",
    "    Atributos:\n",
    "        root_path (str): Ruta del directorio raíz del dataset\n",
    "        category (str): Categoría de producto siendo cargada\n",
    "        transform (callable): Pipeline de transformación de imágenes\n",
    "        class_names (list): Nombres de clases binarias ['normal', 'anomaly']\n",
    "        image_paths (list): Lista de rutas a todas las imágenes cargadas\n",
    "        labels (list): Lista de etiquetas binarias (0=normal, 1=anomalía)\n",
    "        defect_types (list): Lista de nombres originales de tipos de defecto para seguimiento\n",
    "    Retorna:\n",
    "        tuple: (imagen, etiqueta, tipo_defecto) donde:\n",
    "            - imagen: Imagen PIL o tensor transformado\n",
    "            - etiqueta: Etiqueta binaria (0 para normal, 1 para anomalía)\n",
    "            - tipo_defecto: Cadena del tipo de defecto original del dataset MVTec\n",
    "    Ejemplo:\n",
    "        >>> dataset = MVTecTestDataset(\n",
    "        ...     root_path='/ruta/a/mvtec',\n",
    "        ...     category='leather',\n",
    "        ...     transform=transforms.ToTensor()\n",
    "        ... )\n",
    "        >>> imagen, etiqueta, tipo_defecto = dataset[0]\n",
    "    \"\"\"\n",
    "    def __init__(self, root_path, category='leather', transform=None):\n",
    "        self.root_path = root_path\n",
    "        self.category = category\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Solo clases binarias para MVTec: normal vs anomalía\n",
    "        self.class_names = ['normal', 'anomaly']\n",
    "        \n",
    "        self._load_data()\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Cargar imágenes y etiquetas del dataset MVTec desde el directorio de prueba.\n",
    "        Este método recorre la estructura del directorio de prueba y carga las rutas de imágenes,\n",
    "        etiquetas y tipos de defectos para la categoría especificada. Las imágenes en el\n",
    "        subdirectorio 'good' se etiquetan como normales (0), mientras que todos los demás\n",
    "        subdirectorios se etiquetan como anomalías (1).\n",
    "        \n",
    "        El método puebla los siguientes atributos de instancia:\n",
    "        - image_paths: Lista de rutas completas a todos los archivos de imagen\n",
    "        - labels: Lista de etiquetas binarias (0 para normal, 1 para anomalía)\n",
    "        - defect_types: Lista de nombres de tipos de defectos para seguimiento\n",
    "        \n",
    "        Estructura de directorio esperada:\n",
    "        root_path/category/test/\n",
    "        ├── good/           # Imágenes normales (etiqueta = 0)\n",
    "        ├── tipo_defecto1/  # Imágenes anómalas (etiqueta = 1)\n",
    "        ├── tipo_defecto2/  # Imágenes anómalas (etiqueta = 1)\n",
    "        └── ...\n",
    "        \n",
    "        Solo se procesan imágenes PNG. Se imprime información de progreso en consola\n",
    "        mostrando el número de imágenes cargadas para cada tipo de defecto.\n",
    "        \"\"\"\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.defect_types = []  # Para tracking de tipos de defecto\n",
    "        \n",
    "        test_dir = os.path.join(self.root_path, self.category, 'test')\n",
    "        print(f\"Cargando MVTec test desde: {test_dir}\")\n",
    "        \n",
    "        for defect_type in os.listdir(test_dir):\n",
    "            defect_path = os.path.join(test_dir, defect_type)\n",
    "            if os.path.isdir(defect_path):\n",
    "                images = [os.path.join(defect_path, f) for f in os.listdir(defect_path) \n",
    "                         if f.endswith('.png')]\n",
    "                \n",
    "                # MVTec: 'good' = normal (0), todo lo demás = anomalía (1)\n",
    "                label = 0 if defect_type == 'good' else 1\n",
    "                \n",
    "                self.image_paths.extend(images)\n",
    "                self.labels.extend([label] * len(images))\n",
    "                self.defect_types.extend([defect_type] * len(images))\n",
    "                print(f\"  {defect_type}: {len(images)} imágenes → clase {label}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Recuperar un elemento individual del dataset por índice.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Índice del elemento a recuperar del dataset.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: Una tupla que contiene:\n",
    "            - image (torch.Tensor o PIL.Image): Los datos de imagen procesados\n",
    "            - label (any): La etiqueta asociada con la imagen  \n",
    "            - defect_type (any): El tipo de defecto para la imagen\n",
    "            \n",
    "        Nota:\n",
    "            La imagen se carga desde la ruta del archivo, se convierte a formato RGB,\n",
    "            y opcionalmente se transforma si se proporciona una función de transformación.\n",
    "        \"\"\"\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, self.defect_types[idx]  # Incluir tipo de defecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed582bf8",
   "metadata": {},
   "source": [
    "### Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9d1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, lr, wd, num_epochs, device, output_dir, logs_dir, model_name):\n",
    "    \"\"\"\n",
    "    Entrenar un modelo de PyTorch con registro integral y validación.\n",
    "    Esta función realiza bucles de entrenamiento y validación durante un número dado de épocas,\n",
    "    rastrea métricas usando TensorBoard, y guarda el modelo con mejor rendimiento basado en\n",
    "    la precisión de validación.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de PyTorch a ser entrenado\n",
    "        train_loader: DataLoader para datos de entrenamiento\n",
    "        val_loader: DataLoader para datos de validación\n",
    "        lr (float): Tasa de aprendizaje para el optimizador\n",
    "        wd (float): Decaimiento de peso para el optimizador\n",
    "        num_epochs (int): Número de épocas de entrenamiento\n",
    "        device: Dispositivo de PyTorch (CPU o CUDA) para entrenamiento\n",
    "        output_dir (str): Directorio para guardar el mejor checkpoint del modelo\n",
    "        logs_dir (str): Directorio para guardar logs de TensorBoard\n",
    "        model_name (str): Prefijo del nombre para modelo guardado y logs\n",
    "    \n",
    "    Returns:\n",
    "        model: El modelo de PyTorch entrenado\n",
    "    \n",
    "    Notas:\n",
    "        - Usa optimizador AdamW con programador de tasa de aprendizaje Cosine Annealing\n",
    "        - Implementa CrossEntropyLoss para clasificación\n",
    "        - Guarda el modelo con mayor precisión de validación\n",
    "        - Registra pérdida de entrenamiento/validación, precisión y tasa de aprendizaje en TensorBoard\n",
    "        - Incluye registro de histogramas y visualización de imágenes de muestra\n",
    "        - Crea directorio de salida si no existe\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter(log_dir=f'{logs_dir}/{model_name}') # Para tensorboard\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # =============\n",
    "        # Entrenamiento\n",
    "        # =============\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Train\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # ==========\n",
    "        # Validación\n",
    "        # ==========\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Val\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                logits, _ = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # =====================================\n",
    "        # Calcular métricas y registrar logging\n",
    "        # =====================================\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train - Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   - Loss: {val_loss/len(val_loader):.4f}, Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, f'{model_name}.pth'))\n",
    "            print(f\"   Nuevo mejor modelo guardado! Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Guardo los valores en tensorboard\n",
    "        writer.add_scalar('Loss/train', train_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss / len(val_loader), epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "        writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        # Logging de histogramas de pérdidas\n",
    "        writer.add_histogram('Train Loss', train_loss / len(train_loader), epoch)\n",
    "        writer.add_histogram('Val Loss', val_loss / len(val_loader), epoch)\n",
    "        writer.add_histogram('Train Accuracy', train_acc, epoch)\n",
    "        writer.add_histogram('Val Accuracy', val_acc, epoch)\n",
    "        # Logging de imágenes de ejemplo\n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            grid = torchvision.utils.make_grid(images[:16], nrow=4)\n",
    "            writer.add_image('Train Images', grid, epoch)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    print(f\" Entrenamiento completado! Mejor accuracy: {best_accuracy:.2f}%\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe4eb4",
   "metadata": {},
   "source": [
    "### Función de Evaluación Multiclase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1dbf099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, device, class_names, model_name, output_dir):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo de detección de anomalías multiclase de forma integral.\n",
    "    Esta función realiza una evaluación completa de un modelo Vision Transformer (ViT) \n",
    "    multiclase para detección de anomalías, incluyendo precisión de clasificación, \n",
    "    rendimiento de detección de anomalías y generación de visualizaciones.\n",
    "    \n",
    "    Args:\n",
    "        model: El objeto modelo entrenado con método detect_anomaly_hybrid\n",
    "        test_loader: DataLoader de PyTorch que contiene el dataset de prueba (imágenes, etiquetas)\n",
    "        device: Dispositivo de PyTorch (cuda/cpu) para inferencia del modelo\n",
    "        class_names (list): Lista de nombres de clases correspondientes a los índices de clase\n",
    "        model_name (str): Nombre del modelo para organización del directorio de salida\n",
    "        output_dir (str): Directorio base de salida para guardar resultados y visualizaciones\n",
    "        \n",
    "    Returns:\n",
    "        dict: Resultados de evaluación integral que contienen:\n",
    "            - multiclass_accuracy (float): Precisión general de clasificación\n",
    "            - confusion_matrix (list): Matriz de confusión como lista anidada\n",
    "            - anomaly_detection_results (dict): Métricas de ROC AUC, precisión y umbral\n",
    "                para diferentes métodos de puntuación (Híbrido, Similitud Coseno, Clasificación)\n",
    "            - class_distribution (dict): Número de muestras por clase en el conjunto de prueba\n",
    "            - total_samples (int): Número total de muestras de prueba\n",
    "            \n",
    "    La función realiza las siguientes evaluaciones:\n",
    "        1. Métricas de clasificación multiclase (precisión, matriz de confusión, reporte por clase)\n",
    "        2. Detección binaria de anomalías (Normal vs Anomalía) usando múltiples métodos de puntuación\n",
    "        3. Genera visualizaciones:\n",
    "           - Mapa de calor de matriz de confusión\n",
    "           - Comparación de curvas ROC\n",
    "           - Histograma de distribución de puntuaciones\n",
    "           - Gráfico de barras de precisión por clase\n",
    "           - Visualización de ejemplos de clasificación\n",
    "        4. Guarda resumen de resultados como archivo JSON\n",
    "        \n",
    "    Nota:\n",
    "        - Asume que el índice de clase 4 representa muestras \"normales/sin_defectos\"\n",
    "        - Crea estructura de subdirectorio de salida: output_dir/model_name/resultados_multiclase/\n",
    "        - Guarda visualizaciones como archivos PNG con etiquetas en español\n",
    "        - Requiere dependencias sklearn, matplotlib y tqdm\n",
    "    \"\"\"\n",
    "    output_dir = os.path.join(output_dir, model_name)\n",
    "    output_dir = os.path.join(output_dir, 'resultados_multiclase')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Almacenar resultados\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_hybrid_scores = []\n",
    "    all_similarity_scores = []\n",
    "    all_classification_scores = []\n",
    "    all_probs = []\n",
    "    all_images = []\n",
    "    \n",
    "    print(\" Evaluación integral del modelo multi-clase...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluación\"):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            results = model.detect_anomaly_hybrid(images)\n",
    "            \n",
    "            all_predictions.extend(results['predicted_classes'].cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_hybrid_scores.extend(results['anomaly_scores'].cpu().numpy())\n",
    "            all_similarity_scores.extend(results['similarity_scores'].cpu().numpy())\n",
    "            all_classification_scores.extend(results['classification_scores'].cpu().numpy())\n",
    "            all_probs.extend(results['class_probabilities'].cpu().numpy())\n",
    "            all_images.extend(images.cpu().numpy())\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_hybrid_scores = np.array(all_hybrid_scores)\n",
    "    all_similarity_scores = np.array(all_similarity_scores)\n",
    "    all_classification_scores = np.array(all_classification_scores)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    print(f\"\\n DISTRIBUCIÓN DE CLASES EN TEST:\")\n",
    "    print(\"=\" * 60)\n",
    "    unique, counts = np.unique(all_labels, return_counts=True)\n",
    "    for class_id, count in zip(unique, counts):\n",
    "        print(f\"  {class_names[class_id]}: {count} imágenes\")\n",
    "    \n",
    "    # 1. EVALUACIÓN DE CLASIFICACIÓN MULTI-CLASE\n",
    "    print(f\"\\n RESULTADOS DE CLASIFICACIÓN MULTI-CLASE:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    multiclass_accuracy = np.mean(all_predictions == all_labels)\n",
    "    print(f\"Accuracy general: {multiclass_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nReporte detallado por clase:\")\n",
    "    print(classification_report(all_labels, all_predictions, \n",
    "                              target_names=class_names, digits=4))\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # 2. EVALUACIÓN DE DETECCIÓN DE ANOMALÍAS\n",
    "    print(f\"\\n RESULTADOS DE DETECCIÓN DE ANOMALÍAS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Convertir a problema binario: Normal (clase 4) vs Anomalía (clases 0,1,2,3,5)\n",
    "    binary_labels = (all_labels != 4).astype(int)  # 0=normal, 1=anomalía (clase 4 = non_defective en Kaggle)\n",
    "    binary_predictions = (all_predictions != 4).astype(int)\n",
    "    \n",
    "    # Evaluar diferentes métodos de scoring\n",
    "    methods = {\n",
    "        'Hybrid (Paper Method)': all_hybrid_scores,\n",
    "        'Cosine Similarity': all_similarity_scores,\n",
    "        'Classification Confidence': all_classification_scores\n",
    "    }\n",
    "    \n",
    "    results_summary = {}\n",
    "    \n",
    "    if len(np.unique(binary_labels)) > 1:  # Si hay ambas clases\n",
    "        for method_name, scores in methods.items():\n",
    "            roc_auc = roc_auc_score(binary_labels, scores)\n",
    "            avg_precision = average_precision_score(binary_labels, scores)\n",
    "            \n",
    "            # Calcular threshold óptimo\n",
    "            fpr, tpr, thresholds = roc_curve(binary_labels, scores)\n",
    "            optimal_idx = np.argmax(tpr - fpr)\n",
    "            optimal_threshold = thresholds[optimal_idx] if len(thresholds) > optimal_idx else 0.5\n",
    "            \n",
    "            # Accuracy con threshold óptimo\n",
    "            binary_pred = (scores > optimal_threshold).astype(int)\n",
    "            binary_accuracy = np.mean(binary_pred == binary_labels)\n",
    "            \n",
    "            results_summary[method_name] = {\n",
    "                'roc_auc': roc_auc,\n",
    "                'avg_precision': avg_precision,\n",
    "                'binary_accuracy': binary_accuracy,\n",
    "                'optimal_threshold': optimal_threshold\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{method_name}:\")\n",
    "            print(f\"  ROC AUC:           {roc_auc:.4f}\")\n",
    "            print(f\"  Average Precision: {avg_precision:.4f}\")\n",
    "            print(f\"  Binary Accuracy:   {binary_accuracy:.4f}\")\n",
    "            print(f\"  Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    # 3. VISUALIZACIONES\n",
    "    print(f\"\\n Generando visualizaciones...\")\n",
    "    \n",
    "    # Matriz de confusión multi-clase\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title('Matriz de Confusión - Clasificación Multi-Clase \\n(6 Categories)', fontsize=14)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Añadir valores a la matriz\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\", fontweight='bold',\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('Etiqueta verdadera')\n",
    "    plt.xlabel('Etiqueta predicha')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'matriz_de_confusion_multiclase.png'), \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Comparación de métodos de detección de anomalías\n",
    "    if len(np.unique(binary_labels)) > 1:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # ROC Curves\n",
    "        for method_name, scores in methods.items():\n",
    "            fpr, tpr, _ = roc_curve(binary_labels, scores)\n",
    "            auc_score = roc_auc_score(binary_labels, scores)\n",
    "            axes[0].plot(fpr, tpr, linewidth=2, \n",
    "                        label=f'{method_name} (AUC={auc_score:.3f})')\n",
    "        \n",
    "        axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        axes[0].set_xlabel('Tasa de falsos positivos')\n",
    "        axes[0].set_ylabel('Tasa de verdaderos positivos')\n",
    "        axes[0].set_title('Comparación de curvas ROC')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Score distributions\n",
    "        normal_scores = all_hybrid_scores[binary_labels == 0]\n",
    "        anomaly_scores = all_hybrid_scores[binary_labels == 1]\n",
    "        \n",
    "        axes[1].hist(normal_scores, bins=30, alpha=0.7, label='Normal', \n",
    "                    color='green', density=True)\n",
    "        axes[1].hist(anomaly_scores, bins=30, alpha=0.7, label='Anomaly', \n",
    "                    color='red', density=True)\n",
    "        axes[1].axvline(results_summary['Hybrid (Paper Method)']['optimal_threshold'], \n",
    "                       color='black', linestyle='--', linewidth=2,\n",
    "                       label=f\"Threshold: {results_summary['Hybrid (Paper Method)']['optimal_threshold']:.3f}\")\n",
    "        axes[1].set_xlabel('Puntuación de anomalía')\n",
    "        axes[1].set_ylabel('Densidad')\n",
    "        axes[1].set_title('Distribución de la puntuación (método híbrido)')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Class-wise accuracy\n",
    "        class_accuracies = []\n",
    "        for class_id in range(len(class_names)):\n",
    "            mask = all_labels == class_id\n",
    "            if mask.sum() > 0:\n",
    "                class_acc = np.mean(all_predictions[mask] == all_labels[mask])\n",
    "                class_accuracies.append(class_acc)\n",
    "            else:\n",
    "                class_accuracies.append(0)\n",
    "        \n",
    "        bars = axes[2].bar(class_names, class_accuracies, \n",
    "                          color=['red' if acc < 0.8 else 'orange' if acc < 0.9 else 'green' \n",
    "                                for acc in class_accuracies])\n",
    "        axes[2].set_ylabel('Precisión')\n",
    "        axes[2].set_title('Precisión de clasificación por clase')\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Añadir valores en las barras\n",
    "        for bar, acc in zip(bars, class_accuracies):\n",
    "            height = bar.get_height()\n",
    "            axes[2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.suptitle('Análisis del rendimiento de ViT multiclase (enfoque de artículo)', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'evaluacion_integral.png'), \n",
    "                    dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. EJEMPLOS DE CLASIFICACIÓN\n",
    "    visualize_multiclass_examples(all_images, all_labels, all_predictions, \n",
    "                                 all_hybrid_scores, class_names, output_dir)\n",
    "    \n",
    "    # 5. RESUMEN FINAL\n",
    "    # Función para convertir tipos numpy a tipos nativos de Python\n",
    "    def convert_to_native(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_to_native(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_native(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    final_results = {\n",
    "        'multiclass_accuracy': float(multiclass_accuracy),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'anomaly_detection_results': convert_to_native(results_summary),\n",
    "        'class_distribution': {class_names[i]: int(count) for i, count in zip(unique, counts)},\n",
    "        'total_samples': int(len(all_labels))\n",
    "    }\n",
    "    \n",
    "    # Guardar resultados\n",
    "    with open(os.path.join(output_dir, 'results_summary.json'), 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca5074",
   "metadata": {},
   "source": [
    "### Funciones para Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c5c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multiclass_examples(images, labels, predictions, scores, class_names, output_dir):\n",
    "    \"\"\"\n",
    "    Visualiza ejemplos de clasificación multiclase mostrando imágenes con sus etiquetas verdaderas y predicciones.\n",
    "    Crea una visualización en forma de grilla donde cada fila representa una clase y cada columna muestra\n",
    "    hasta 4 ejemplos de esa clase. Las predicciones correctas se muestran con borde verde y las incorrectas\n",
    "    con borde rojo.\n",
    "    Args:\n",
    "        images (array-like): Array de imágenes a visualizar. Se espera formato (N, C, H, W) o (N, H, W, C).\n",
    "        labels (array-like): Etiquetas verdaderas correspondientes a cada imagen.\n",
    "        predictions (array-like): Predicciones del modelo para cada imagen.\n",
    "        scores (array-like): Puntuaciones de confianza para cada predicción.\n",
    "        class_names (list): Lista con los nombres de las clases en orden de índices.\n",
    "        output_dir (str): Directorio donde guardar la imagen de visualización.\n",
    "    Returns:\n",
    "        None: La función guarda la visualización como archivo PNG y no retorna valores.\n",
    "    Note:\n",
    "        - Las imágenes se desnormalizan usando los valores estándar de ImageNet\n",
    "        - Se asume que las imágenes están normalizadas con media [0.485, 0.456, 0.406] \n",
    "          y desviación estándar [0.229, 0.224, 0.225]\n",
    "        - El archivo se guarda como 'ejemplos_multiclase.png' en el directorio especificado\n",
    "        - Si hay menos de 4 ejemplos para una clase, se muestran todos los disponibles\n",
    "        - Si no hay ejemplos para una clase, se muestra un mensaje indicándolo\n",
    "    Raises:\n",
    "        Exception: Captura y reporta cualquier error durante el proceso de visualización\n",
    "    \"\"\"\n",
    "    print(f\" Creando ejemplos de clasificación multi-clase...\")\n",
    "    \n",
    "    try:\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "        predictions = np.array(predictions)\n",
    "        scores = np.array(scores)\n",
    "        \n",
    "        # Crear figura grande para todas las clases\n",
    "        fig, axes = plt.subplots(len(class_names), 4, figsize=(20, 4*len(class_names)))\n",
    "        if len(class_names) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            # Encontrar ejemplos de esta clase\n",
    "            class_mask = labels == class_idx\n",
    "            class_indices = np.where(class_mask)[0]\n",
    "            \n",
    "            if len(class_indices) > 0:\n",
    "                # Seleccionar hasta 4 ejemplos\n",
    "                selected_indices = class_indices[:4] if len(class_indices) >= 4 else class_indices\n",
    "                \n",
    "                for i, idx in enumerate(selected_indices):\n",
    "                    img = images[idx].copy()\n",
    "                    \n",
    "                    # Procesar imagen\n",
    "                    if len(img.shape) == 3 and img.shape[0] == 3:\n",
    "                        img = np.transpose(img, (1, 2, 0))\n",
    "                    \n",
    "                    # Desnormalizar\n",
    "                    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "                    img = np.clip(img, 0, 1)\n",
    "                    \n",
    "                    # Determinar color del borde basado en correctness\n",
    "                    is_correct = predictions[idx] == labels[idx]\n",
    "                    border_color = 'green' if is_correct else 'red'\n",
    "                    \n",
    "                    axes[class_idx, i].imshow(img)\n",
    "                    axes[class_idx, i].set_title(\n",
    "                        f\"True: {class_name}\\n\"\n",
    "                        f\"Pred: {class_names[predictions[idx]]}\\n\"\n",
    "                        f\"Score: {scores[idx]:.3f}\",\n",
    "                        color=border_color, fontsize=9\n",
    "                    )\n",
    "                    axes[class_idx, i].axis('off')\n",
    "                \n",
    "                # Rellenar espacios vacíos\n",
    "                for i in range(len(selected_indices), 4):\n",
    "                    axes[class_idx, i].text(0.5, 0.5, 'No more\\nexamples', \n",
    "                                          ha='center', va='center', \n",
    "                                          transform=axes[class_idx, i].transAxes)\n",
    "                    axes[class_idx, i].axis('off')\n",
    "            else:\n",
    "                # No hay ejemplos de esta clase\n",
    "                for i in range(4):\n",
    "                    axes[class_idx, i].text(0.5, 0.5, f'No examples\\nof {class_name}', \n",
    "                                          ha='center', va='center', \n",
    "                                          transform=axes[class_idx, i].transAxes)\n",
    "                    axes[class_idx, i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Ejemplos de clasificación de múltiples clases\\n(Verde=Correcto, Rojo=Incorrecto)',\n",
    "                                fontsize=18, y=0.99)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        examples_file = os.path.join(output_dir, 'ejemplos_multiclase.png')\n",
    "        plt.savefig(examples_file, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"  ✓ Ejemplos guardados: {examples_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error creando ejemplos: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb2fb0",
   "metadata": {},
   "source": [
    "### Función para Validación Visual en MVTec AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7befe38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_validation_mvtec(model, mvtec_loader, device, model_name, output_dir):\n",
    "    \"\"\"\n",
    "    Realiza validación visual de detección de anomalías en el dataset MVTec AD siguiendo un enfoque de evaluación cualitativa.\n",
    "    Esta función genera salidas visuales integrales incluyendo mapas de calor, superposiciones y resúmenes estadísticos\n",
    "    para demostrar la capacidad del modelo para detectar y localizar defectos en muestras de cuero del dataset MVTec AD\n",
    "    usando características aprendidas del dataset original de clasificación de defectos en cuero.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado de detección de anomalías con métodos:\n",
    "               - detect_anomaly_hybrid(): Devuelve puntuaciones de anomalía y métricas relacionadas\n",
    "               - extract_attention_maps(): Devuelve mapas de atención para visualización\n",
    "        mvtec_loader (DataLoader): DataLoader de PyTorch para el dataset MVTec AD\n",
    "        device (torch.device): Dispositivo para ejecutar inferencia (CPU/GPU)\n",
    "        model_name (str): Nombre del modelo para organización del directorio de salida\n",
    "        output_dir (str): Directorio base donde se guardarán los resultados de validación\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario que contiene estadísticas de validación:\n",
    "            - samples_processed (int): Número de muestras procesadas exitosamente\n",
    "            - normal_samples (int): Cantidad de muestras normales en el lote\n",
    "            - anomaly_samples (int): Cantidad de muestras anómalas en el lote\n",
    "            - output_dir (str): Ruta al directorio que contiene las salidas generadas\n",
    "            - correct_detections (int): Número de detecciones correctas de anomalías\n",
    "            - false_positives (int): Número de detecciones falso positivas\n",
    "            - false_negatives (int): Número de detecciones falso negativas\n",
    "            - true_negatives (int): Número de detecciones verdadero negativas\n",
    "            \n",
    "    Salidas Generadas:\n",
    "        - validacion_visual_mvtec.png: Visualización principal de 4 columnas mostrando:\n",
    "          * Imágenes originales con tipos de defectos\n",
    "          * Mapas de calor de atención con puntuaciones de anomalía\n",
    "          * Superposiciones de mapas de calor en imágenes originales\n",
    "          * Análisis de detección e interpretación de puntuaciones\n",
    "        - imagenes_ejemplo_mvtec.png: Grilla de referencia de imágenes de muestra originales\n",
    "        - visual_validation_summary.txt: Resumen textual detallado con metodología y resultados\n",
    "        \n",
    "    Metodología:\n",
    "        1. Extrae características usando backbone ViT entrenado en dataset de defectos en cuero\n",
    "        2. Compara características con características 'normales' almacenadas usando similitud coseno\n",
    "        3. Genera puntuaciones de anomalía (1 - similitud_máxima)\n",
    "        4. Crea mapas de calor de atención para localización de anomalías\n",
    "        5. Superpone mapas de calor en imágenes originales para interpretación\n",
    "        \n",
    "    Nota:\n",
    "        Sigue el enfoque del artículo de usar MVTec AD para confirmación visual en lugar de\n",
    "        evaluación cuantitativa. El enfoque está en demostrar la capacidad de generalización del modelo\n",
    "        a diferentes tipos de defectos en cuero en lugar de lograr métricas numéricas específicas.\n",
    "        La función incluye manejo integral de errores y mecanismos de respaldo para asegurar\n",
    "        procesamiento robusto incluso con lotes problemáticos o muestras individuales.\n",
    "        \n",
    "    Interpretación de Colores:\n",
    "        - Áreas rojas/amarillas: Alta probabilidad de anomalía (puntuación > 0.5)\n",
    "        - Áreas naranjas: Probabilidad media de anomalía (0.3-0.5)\n",
    "        - Áreas azules/verdes: Baja probabilidad de anomalía (< 0.3)\n",
    "    \"\"\"\n",
    "    \n",
    "    output_dir = os.path.join(output_dir, model_name)\n",
    "    output_dir = os.path.join(output_dir, 'resultados_mvtec_visual')  # Cambio a mismo nombre que función original\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    print(\" Generando validación visual ...\")\n",
    "    print(\" Siguiendo el enfoque del paper: validación cualitativa únicamente\")\n",
    "\n",
    "    sample_data = []\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(mvtec_loader):\n",
    "                print(f\" Procesando batch {batch_idx}...\")\n",
    "                \n",
    "                if len(batch_data) == 3:\n",
    "                    images, labels, defect_types = batch_data\n",
    "                else:\n",
    "                    images, labels = batch_data\n",
    "                    defect_types = ['unknown'] * len(labels)\n",
    "\n",
    "                images = images.to(device)\n",
    "                #print(f\"   Imágenes shape: {images.shape}\")\n",
    "\n",
    "                # Procesar batch con manejo de errores\n",
    "                try:\n",
    "                    results = model.detect_anomaly_hybrid(images)\n",
    "                    attention_maps = model.extract_attention_maps(images)\n",
    "                    \n",
    "                    if len(attention_maps) != images.size(0):\n",
    "                        print(f\" Mismatch: {len(attention_maps)} attention maps vs {images.size(0)} imágenes\")\n",
    "                        # Completar con mapas neutros\n",
    "                        while len(attention_maps) < images.size(0):\n",
    "                            attention_maps.append(np.ones((14, 14)) * 0.5)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\" Error procesando batch: {e}\")\n",
    "                    # Crear resultados de fallback\n",
    "                    batch_size = images.size(0)\n",
    "                    results = {\n",
    "                        'anomaly_scores': torch.ones(batch_size) * 0.5,\n",
    "                        'similarity_scores': torch.ones(batch_size) * 0.5,\n",
    "                        'classification_scores': torch.ones(batch_size) * 0.5,\n",
    "                        'normal_class_prob': torch.ones(batch_size) * 0.5\n",
    "                    }\n",
    "                    attention_maps = [np.ones((14, 14)) * 0.5] * batch_size\n",
    "\n",
    "                # Recopilar datos\n",
    "                for i in range(images.size(0)):\n",
    "                    try:\n",
    "                        sample_data.append({\n",
    "                            'image': images[i:i+1],\n",
    "                            'label': labels[i].item(),\n",
    "                            'defect_type': defect_types[i] if isinstance(defect_types, list) else f\"type_{labels[i].item()}\",\n",
    "                            'anomaly_score': results['anomaly_scores'][i].item() if torch.is_tensor(results['anomaly_scores']) else 0.5,\n",
    "                            'similarity_score': results['similarity_scores'][i].item() if torch.is_tensor(results['similarity_scores']) else 0.5,\n",
    "                            'classification_score': results['classification_scores'][i].item() if torch.is_tensor(results['classification_scores']) else 0.5,\n",
    "                            'normal_class_prob': results['normal_class_prob'][i].item() if torch.is_tensor(results['normal_class_prob']) else 0.5,\n",
    "                            'attention_map': attention_maps[i] if i < len(attention_maps) else np.ones((14, 14)) * 0.5\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\" Error recopilando muestra {i}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    if len(sample_data) >= 12:\n",
    "                        break\n",
    "\n",
    "                if len(sample_data) >= 12:\n",
    "                    break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error general: {e}\")\n",
    "        if len(sample_data) == 0:\n",
    "            return {'error': str(e)}\n",
    "\n",
    "    print(f\" Procesando {len(sample_data)} imágenes de ejemplo...\")\n",
    "\n",
    "    # 1. CREAR VISUALIZACIÓN PRINCIPAL (4 columnas)\n",
    "    n_samples = len(sample_data)\n",
    "    fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4*n_samples))\n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for idx, data in enumerate(sample_data):\n",
    "        try:\n",
    "            # Procesar imagen\n",
    "            img_np = data['image'].squeeze().cpu().numpy()\n",
    "            if len(img_np.shape) == 3 and img_np.shape[0] == 3:\n",
    "                img_np = np.transpose(img_np, (1, 2, 0))\n",
    "\n",
    "            img_display = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "            # Crear heatmap seguro\n",
    "            heatmap_overlay, heatmap_colored, attention_norm = create_heatmap(\n",
    "                img_display, data['attention_map'], data['anomaly_score']\n",
    "            )\n",
    "\n",
    "            # Visualización\n",
    "            axes[idx, 0].imshow(img_display)\n",
    "            axes[idx, 0].set_title(f'Imagen Original\\n{data[\"defect_type\"]}', fontsize=10, fontweight='bold')\n",
    "            axes[idx, 0].axis('off')\n",
    "\n",
    "            axes[idx, 1].imshow(data['attention_map'], cmap='viridis')\n",
    "            axes[idx, 1].set_title(f'Mapa de calor de anomalías\\nPuntuación: {data[\"anomaly_score\"]:.3f}', fontsize=10)\n",
    "            axes[idx, 1].axis('off')\n",
    "\n",
    "            axes[idx, 2].imshow(heatmap_overlay)\n",
    "            axes[idx, 2].set_title('Superposición de mapa de calor', fontsize=10)\n",
    "            axes[idx, 2].axis('off')\n",
    "\n",
    "            # Columna 4: Información y análisis (como función original)\n",
    "            status = 'ANOMALY' if data['label'] == 1 else 'NORMAL'\n",
    "            status_color = 'red' if data['label'] == 1 else 'green'\n",
    "\n",
    "            # Determinar si fue detectado correctamente\n",
    "            detected_as_anomaly = data['anomaly_score'] > 0.5\n",
    "            detection_correct = (detected_as_anomaly and data['label'] == 1) or (not detected_as_anomaly and data['label'] == 0)\n",
    "            detection_status = 'CORRECT' if detection_correct else 'INCORRECT'\n",
    "            detection_color = 'green' if detection_correct else 'red'\n",
    "\n",
    "            # Texto informativo (como función original)\n",
    "            axes[idx, 3].text(0.05, 0.85, f'Ground Truth:', \n",
    "                             transform=axes[idx, 3].transAxes, fontsize=9, fontweight='bold')\n",
    "            axes[idx, 3].text(0.05, 0.75, f'{status}', \n",
    "                             transform=axes[idx, 3].transAxes, fontsize=10,\n",
    "                             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=status_color, alpha=0.3))\n",
    "\n",
    "            axes[idx, 3].text(0.05, 0.55, f'Detection:', \n",
    "                             transform=axes[idx, 3].transAxes, fontsize=9, fontweight='bold')\n",
    "            axes[idx, 3].text(0.05, 0.45, f'{detection_status}', \n",
    "                             transform=axes[idx, 3].transAxes, fontsize=10,\n",
    "                             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=detection_color, alpha=0.3))\n",
    "\n",
    "            axes[idx, 3].text(0.05, 0.25, f'Anomaly Score:', \n",
    "                             transform=axes[idx, 3].transAxes, fontsize=9, fontweight='bold')\n",
    "            axes[idx, 3].text(0.05, 0.15, f'{data[\"anomaly_score\"]:.4f}', \n",
    "                             transform=axes[idx, 3].transAxes, fontsize=10,\n",
    "                             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "            # Interpretación del score (como función original)\n",
    "            if data['anomaly_score'] > 0.7:\n",
    "                interpretation = \"High Anomaly\"\n",
    "                interp_color = 'red'\n",
    "            elif data['anomaly_score'] > 0.4:\n",
    "                interpretation = \"Medium Anomaly\"\n",
    "                interp_color = 'orange'\n",
    "            else:\n",
    "                interpretation = \"Low/Normal\"\n",
    "                interp_color = 'green'\n",
    "\n",
    "            axes[idx, 3].text(0.05, 0.05, f'{interpretation}', \n",
    "                             transform=axes[idx, 3].transAxes, fontsize=9,\n",
    "                             bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=interp_color, alpha=0.2))\n",
    "\n",
    "            axes[idx, 3].axis('off')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error visualizando muestra {idx}: {e}\")\n",
    "            for col in range(4):\n",
    "                axes[idx, col].text(0.5, 0.5, f'Error\\n{e}', ha='center', va='center')\n",
    "                axes[idx, col].axis('off')\n",
    "\n",
    "    # Título general y configuración (como función original)\n",
    "    plt.suptitle(\n",
    "        \"Validación visual de MVTec AD - Mapas de calor de detección de anomalías\\n\"\n",
    "        + \"(Siguiendo el enfoque del artículo: Solo evaluación cualitativa)\\n\"\n",
    "        + \"Rojo/Amarillo = Alta probabilidad de anomalía, Azul = Baja probabilidad de anomalía\",\n",
    "        fontsize=14,\n",
    "        y=0.98,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar visualización principal\n",
    "    visual_file = os.path.join(output_dir, 'validacion_visual_mvtec.png')\n",
    "    plt.savefig(visual_file, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"    Validación visual guardada: {visual_file}\")\n",
    "\n",
    "    # 2. CREAR VISUALIZACIÓN ADICIONAL DE REFERENCIA (del segundo código)\n",
    "    fig2, axes2 = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    for idx in range(min(12, len(sample_data))):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "\n",
    "        img = sample_data[idx]['image']\n",
    "        img_np = img.squeeze().cpu().numpy()\n",
    "        if len(img_np.shape) == 3 and img_np.shape[0] == 3:\n",
    "            img_np = np.transpose(img_np, (1, 2, 0))\n",
    "        img_display = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "        # Mostrar imagen original con título informativo\n",
    "        axes2[row, col].imshow(img_display)\n",
    "        status = 'ANOMALY' if sample_data[idx]['label'] == 1 else 'NORMAL'\n",
    "        axes2[row, col].set_title(f'{sample_data[idx][\"defect_type\"]}\\n{status}', fontsize=10)\n",
    "        axes2[row, col].axis('off')\n",
    "\n",
    "    plt.suptitle('Imágenes de muestra de MVTec AD\\n(Imágenes originales de referencia)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    reference_file = os.path.join(output_dir, 'imagenes_ejemplo_mvtec.png')\n",
    "    plt.savefig(reference_file, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"    Imágenes de referencia guardadas: {reference_file}\")\n",
    "\n",
    "    # 3. CALCULAR ESTADÍSTICAS DE DETECCIÓN (del segundo código)\n",
    "    sample_labels = [data['label'] for data in sample_data]\n",
    "    sample_names = [data['defect_type'] for data in sample_data]\n",
    "    \n",
    "    normal_count = sum(1 for label in sample_labels if label == 0)\n",
    "    anomaly_count = sum(1 for label in sample_labels if label == 1)\n",
    "\n",
    "    # Calcular estadísticas de detección\n",
    "    correct_detections = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    for data in sample_data:\n",
    "        try:\n",
    "            predicted_anomaly = data['anomaly_score'] > 0.5\n",
    "            actual_anomaly = data['label'] == 1\n",
    "\n",
    "            if predicted_anomaly and actual_anomaly:\n",
    "                correct_detections += 1\n",
    "            elif predicted_anomaly and not actual_anomaly:\n",
    "                false_positives += 1\n",
    "            elif not predicted_anomaly and actual_anomaly:\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Error calculando estadísticas: {e}\")\n",
    "\n",
    "    # 4. CREAR RESUMEN TEXTUAL DETALLADO (del segundo código)\n",
    "    summary_text = f\"\"\"Resumen de Validación Visual MVTec AD\n",
    "{'=' * 50}\n",
    "\n",
    "Siguiendo el enfoque del artículo: solo evaluación cualitativa\n",
    "Esta validación demuestra la capacidad del modelo para detectar y localizar\n",
    "defectos en el dataset MVTec AD usando características aprendidas del dataset\n",
    "original de clasificación de defectos en cuero.\n",
    "\n",
    "Información del Dataset:\n",
    "- Muestras procesadas: {len(sample_data)}\n",
    "- Muestras normales: {normal_count}\n",
    "- Muestras anómalas: {anomaly_count}\n",
    "- Tipos de muestra: {', '.join(set(sample_names))}\n",
    "\n",
    "Resultados de Evaluación Visual:\n",
    "- Detecciones correctas: {correct_detections}\n",
    "- Falsos positivos: {false_positives}\n",
    "- Falsos negativos: {false_negatives}\n",
    "- Verdaderos negativos: {true_negatives}\n",
    "\n",
    "Interpretación del Mapa de Calor:\n",
    "- Áreas rojas/amarillas: Alta probabilidad de anomalía (puntuación > 0.5)\n",
    "- Áreas naranjas: Probabilidad media de anomalía (0.3-0.5)\n",
    "- Áreas azules/verdes: Baja probabilidad de anomalía (< 0.3)\n",
    "- La superposición combina la imagen original con el mapa de calor de anomalías\n",
    "\n",
    "Metodología:\n",
    "1. Extraer características usando el backbone ViT entrenado en el dataset de defectos de cuero\n",
    "2. Comparar características con características 'normales' almacenadas usando similitud coseno\n",
    "3. Generar puntuaciones de anomalía (1 - similitud_máxima)\n",
    "4. Crear mapas de calor para visualizar regiones anómalas\n",
    "5. Superponer mapas de calor en imágenes originales para interpretación\n",
    "\n",
    "Nota: Esto sigue la metodología del artículo de usar MVTec AD para\n",
    "confirmación visual en lugar de evaluación cuantitativa. El enfoque está en\n",
    "demostrar la capacidad del modelo para generalizar a diferentes tipos\n",
    "de defectos en cuero, no en lograr métricas de rendimiento numérico específicas.\n",
    "\n",
    "Archivos Generados:\n",
    "- validacion_visual_mvtec.png: Visualización principal con mapas de calor\n",
    "- imagenes_ejemplo_mvtec.png: Imágenes de referencia\n",
    "- visual_validation_summary.txt: Este archivo de resumen\n",
    "\n",
    "Conclusión:\n",
    "La validación visual demuestra la capacidad del modelo para detectar anomalías\n",
    "en muestras de cuero de MVTec AD usando características aprendidas del dataset\n",
    "original, siguiendo el enfoque de evaluación cualitativa descrito en el artículo.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Guardar resumen\n",
    "    summary_file = os.path.join(output_dir, 'visual_validation_summary.txt')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(summary_text)\n",
    "\n",
    "    print(f\"    Resumen detallado guardado: {summary_file}\")\n",
    "    print(f\"\\n VALIDACIÓN VISUAL COMPLETADA:\")\n",
    "    print(f\"   - {len(sample_data)} imágenes procesadas\")\n",
    "    print(f\"   - Mapas de calor generados según metodología del paper\")\n",
    "    print(f\"   - Sin métricas cuantitativas (siguiendo el paper)\")\n",
    "    print(f\"   - Enfoque en demostración visual de capacidades\")\n",
    "    print(f\"   - Procesamiento con manejo de errores\")\n",
    "\n",
    "    # 5. DEVOLVER ESTADÍSTICAS COMPLETAS (como función original)\n",
    "    return {\n",
    "        'samples_processed': len(sample_data),\n",
    "        'normal_samples': normal_count,\n",
    "        'anomaly_samples': anomaly_count,\n",
    "        'output_dir': output_dir,\n",
    "        'correct_detections': correct_detections,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'true_negatives': true_negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ae647",
   "metadata": {},
   "source": [
    "### Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bbcd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setear_semilla(seed=42):\n",
    "    \"\"\"\n",
    "    Establece una semilla fija para garantizar la reproducibilidad de los resultados.\n",
    "    Esta función configura las semillas de todos los generadores de números aleatorios\n",
    "    utilizados por NumPy, PyTorch (CPU y GPU), y Python, además de configurar\n",
    "    CuDNN para comportamiento determinístico.\n",
    "    Args:\n",
    "        seed (int, optional): Valor de la semilla a utilizar. Por defecto es 42.\n",
    "    Returns:\n",
    "        None\n",
    "    Note:\n",
    "        - Configura torch.backends.cudnn.deterministic=True para garantizar\n",
    "          resultados reproducibles en GPU, aunque esto puede reducir el rendimiento.\n",
    "        - Desactiva torch.backends.cudnn.benchmark para evitar optimizaciones\n",
    "          no determinísticas.\n",
    "        - Establece PYTHONHASHSEED para garantizar hashing determinístico.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Si se está ejecutando en el backend CuDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Establece una semilla fija para el hash de Python\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb273b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(image_np, attention_map, anomaly_score):\n",
    "    \"\"\"\n",
    "    Crea una superposición de mapa de calor en una imagen basada en un mapa de atención y puntuación de anomalía.\n",
    "    Esta función toma una imagen de entrada, un mapa de atención y una puntuación de anomalía para generar\n",
    "    una superposición de mapa de calor coloreado que visualiza áreas de interés o anomalías en la imagen.\n",
    "    La función incluye manejo robusto de errores y mecanismos de respaldo para entradas inválidas.\n",
    "    \n",
    "    Args:\n",
    "        image_np (numpy.ndarray): Imagen de entrada como array de numpy con forma (H, W, C) o (H, W).\n",
    "                                 Los valores deben estar normalizados entre 0 y 1.\n",
    "        attention_map (numpy.ndarray o None): Array 2D de mapa de atención, típicamente de forma (14, 14).\n",
    "                                             Si es None o inválido, se usará un mapa neutro.\n",
    "        anomaly_score (float): Puntuación de anomalía entre 0 y 1 que modula la intensidad del mapa de calor.\n",
    "                              Valores más altos crean mapas de calor más intensos.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Una tupla que contiene tres elementos:\n",
    "            - overlay (numpy.ndarray): La imagen original con superposición de mapa de calor aplicada,\n",
    "                                     forma (H, W, 3), valores recortados a [0, 1].\n",
    "            - heatmap_colored (numpy.ndarray): El mapa de calor coloreado sin la imagen original,\n",
    "                                             forma (H, W, 3), valores en [0, 1].\n",
    "            - attention_norm (numpy.ndarray): El mapa de atención normalizado y redimensionado,\n",
    "                                            forma (H, W), valores en [0, 1].\n",
    "    \n",
    "    Notas:\n",
    "        - La función redimensiona automáticamente el mapa de atención para coincidir con las dimensiones de la imagen\n",
    "        - Usa interpolación cúbica para un redimensionado más suave\n",
    "        - Aplica mapa de colores jet para visualización del mapa de calor\n",
    "        - Incluye mecanismos de validación y respaldo para robustez\n",
    "        - La superposición usa combinación ponderada: 65% imagen original + 35% mapa de calor\n",
    "        - En caso de errores, devuelve la imagen original con mapa de calor neutro\n",
    "    \n",
    "    Ejemplo:\n",
    "        >>> import numpy as np\n",
    "        >>> imagen = np.random.rand(224, 224, 3)\n",
    "        >>> atencion = np.random.rand(14, 14)\n",
    "        >>> puntuacion = 0.8\n",
    "        >>> superposicion, mapa_calor, atencion_norm = create_heatmap(imagen, atencion, puntuacion)\n",
    "    \"\"\"\n",
    "    h, w = image_np.shape[:2]\n",
    "    \n",
    "    # Validar attention_map\n",
    "    if attention_map is None:\n",
    "        print(\" Attention map es None, usando mapa neutro\")\n",
    "        attention_map = np.ones((14, 14)) * 0.5\n",
    "    \n",
    "    if not isinstance(attention_map, np.ndarray):\n",
    "        print(f\" Attention map no es numpy array: {type(attention_map)}\")\n",
    "        attention_map = np.ones((14, 14)) * 0.5\n",
    "    \n",
    "    # Asegurar forma correcta\n",
    "    if len(attention_map.shape) != 2:\n",
    "        print(f\" Attention map no es 2D: {attention_map.shape}\")\n",
    "        attention_map = np.ones((14, 14)) * 0.5\n",
    "    \n",
    "    try:\n",
    "        # Redimensionar\n",
    "        attention_resized = cv2.resize(attention_map, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Normalizar\n",
    "        att_min, att_max = attention_resized.min(), attention_resized.max()\n",
    "        if att_max > att_min:\n",
    "            attention_norm = (attention_resized - att_min) / (att_max - att_min)\n",
    "        else:\n",
    "            attention_norm = np.ones_like(attention_resized) * 0.5\n",
    "        \n",
    "        # Aplicar anomaly score\n",
    "        heatmap_intensity = attention_norm * (0.3 + 0.7 * anomaly_score)\n",
    "        \n",
    "        # Colormap\n",
    "        heatmap_colored = plt.cm.jet(heatmap_intensity)[:, :, :3]\n",
    "        \n",
    "        # Overlay\n",
    "        alpha, beta = 0.65, 0.35\n",
    "        overlay = alpha * image_np + beta * heatmap_colored\n",
    "        overlay = np.clip(overlay, 0, 1)\n",
    "        \n",
    "        return overlay, heatmap_colored, attention_norm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error creando heatmap: {e}\")\n",
    "        # Fallback\n",
    "        neutral_heatmap = np.ones((h, w, 3)) * 0.5\n",
    "        return image_np, neutral_heatmap, np.ones((h, w)) * 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e2b5c",
   "metadata": {},
   "source": [
    "### Cuerpo principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9636a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seteo la semilla para reproducibilidad\n",
    "setear_semilla(42)\n",
    "\n",
    "# Variables globales de configuración\n",
    "ORIGINAL_DATASET_PATH = 'data/leather_defect_classification/'  # Dataset para entrenamiento \n",
    "MVTEC_DATASET_PATH = 'data/mvtec/'  # Dataset MVTec AD para validación visual (cualitativa)\n",
    "\n",
    "# Rutas para guardar resultados y modelos\n",
    "output_dir = 'models/'  # Ruta para guardar el modelo entrenado\n",
    "reports_path = 'reports/'  # Ruta para guardar los reportes e imágenes\n",
    "logs_dir = 'logs/' # Directorio de logs para tensorboard\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# Clases del paper (orden corregido para el dataset de Kaggle)\n",
    "CLASS_NAMES = [\n",
    "    'folding_marks',    # 0 → \"Folding marks\" \n",
    "    'grain_off',        # 1 → \"Grain off\"\n",
    "    'growth_marks',     # 2 → \"Growth marks\"\n",
    "    'loose_grain',      # 3 → \"loose grains\" (plural en Kaggle)\n",
    "    'non_defective',    # 4 → \"non defective\" \n",
    "    'pinhole'           # 5 → \"pinhole\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c74e6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorios de salida en caso de que no existan\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(reports_path, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ac32d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CONFIGURACIÓN MULTI-CLASE (DATASET KAGGLE)\n",
      "======================================================================\n",
      "Dataset original (entrenamiento): data/leather_defect_classification/\n",
      "Dataset MVTec (validación visual): data/mvtec/\n",
      "Batch size: 16\n",
      "Épocas: 15\n",
      "Clases del dataset Kaggle:\n",
      "  0: folding_marks <- 'Folding marks'\n",
      "  1: grain_off <- 'Grain off'\n",
      "  2: growth_marks <- 'Growth marks'\n",
      "  3: loose_grain <- 'loose grains'\n",
      "  4: non_defective <- 'non defective'\n",
      "  5: pinhole <- 'pinhole'\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Imprimir configuración del dataset para entrenamiento y validación\n",
    "print(\" CONFIGURACIÓN MULTI-CLASE (DATASET KAGGLE)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset original (entrenamiento): {ORIGINAL_DATASET_PATH}\")\n",
    "print(f\"Dataset MVTec (validación visual): {MVTEC_DATASET_PATH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Épocas: {NUM_EPOCHS}\")\n",
    "print(f\"Clases del dataset Kaggle:\")\n",
    "kaggle_folders = ['Folding marks', 'Grain off', 'Growth marks', 'loose grains', 'non defective', 'pinhole']\n",
    "\n",
    "for i, (class_name, folder_name) in enumerate(zip(CLASS_NAMES, kaggle_folders)):\n",
    "    print(f\"  {i}: {class_name} <- '{folder_name}'\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b268fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos transformaciones para el dataset de entrenamiento\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4ed50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Si tenemos disponible GPU, lo usamos\n",
    "# Chequeamos si tenemos disponible GPU (CUDA)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# Chequeamos si tenemos disponible aceleración por hardware en un chip de Apple (MPS)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "# Por defecto usamos CPU\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\" Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3015fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cargando datasets...\n",
      "\n",
      " DATASET ORIGINAL (ENTRENAMIENTO - 6 CLASES):\n",
      "Cargando desde: data/leather_defect_classification/\n",
      "Carpetas esperadas: ['folding_marks', 'grain_off', 'growth_marks', 'loose_grains', 'non_defective', 'pinhole']\n",
      "   folding_marks: 600 imágenes → clase 0 (folding_marks)\n",
      "   grain_off: 600 imágenes → clase 1 (grain_off)\n",
      "   growth_marks: 600 imágenes → clase 2 (growth_marks)\n",
      "   loose_grains: 600 imágenes → clase 3 (loose_grain)\n",
      "   non_defective: 600 imágenes → clase 4 (non_defective)\n",
      "   pinhole: 600 imágenes → clase 5 (pinhole)\n",
      "\n",
      " DIVISIÓN TRAIN/VALIDATION:\n",
      "Modo: Entrenamiento\n",
      "Total imágenes: 2880\n",
      "  folding_marks: 480 imágenes\n",
      "  grain_off: 480 imágenes\n",
      "  growth_marks: 480 imágenes\n",
      "  loose_grain: 480 imágenes\n",
      "  non_defective: 480 imágenes\n",
      "  pinhole: 480 imágenes\n",
      "Cargando desde: data/leather_defect_classification/\n",
      "Carpetas esperadas: ['folding_marks', 'grain_off', 'growth_marks', 'loose_grains', 'non_defective', 'pinhole']\n",
      "   folding_marks: 600 imágenes → clase 0 (folding_marks)\n",
      "   grain_off: 600 imágenes → clase 1 (grain_off)\n",
      "   growth_marks: 600 imágenes → clase 2 (growth_marks)\n",
      "   loose_grains: 600 imágenes → clase 3 (loose_grain)\n",
      "   non_defective: 600 imágenes → clase 4 (non_defective)\n",
      "   pinhole: 600 imágenes → clase 5 (pinhole)\n",
      "\n",
      " DIVISIÓN TRAIN/VALIDATION:\n",
      "Modo: Validación\n",
      "Total imágenes: 720\n",
      "  folding_marks: 120 imágenes\n",
      "  grain_off: 120 imágenes\n",
      "  growth_marks: 120 imágenes\n",
      "  loose_grain: 120 imágenes\n",
      "  non_defective: 120 imágenes\n",
      "  pinhole: 120 imágenes\n"
     ]
    }
   ],
   "source": [
    "# Crear datasets de entrenamiento y validación\n",
    "print(\" Cargando datasets...\")\n",
    "print(\"\\n DATASET ORIGINAL (ENTRENAMIENTO - 6 CLASES):\")\n",
    "\n",
    "# Dataset original del paper para entrenamiento\n",
    "train_dataset = LeatherDefectDataset(\n",
    "    root_path=ORIGINAL_DATASET_PATH,\n",
    "    is_train=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = LeatherDefectDataset(\n",
    "    root_path=ORIGINAL_DATASET_PATH,\n",
    "    is_train=False,  # Usa validation del dataset original\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a412c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATASET MVTEC (VALIDACIÓN VISUAL):\n",
      "Cargando MVTec test desde: data/mvtec/leather/test\n",
      "  glue: 19 imágenes → clase 1\n",
      "  fold: 17 imágenes → clase 1\n",
      "  color: 19 imágenes → clase 1\n",
      "  good: 32 imágenes → clase 0\n",
      "  poke: 18 imágenes → clase 1\n",
      "  cut: 19 imágenes → clase 1\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset de prueba para validación visual con MVTec AD\n",
    "print(f\"\\n DATASET MVTEC (VALIDACIÓN VISUAL):\")\n",
    "\n",
    "# MVTec AD solo para validación visual (siguiendo el paper)\n",
    "mvtec_test_dataset = MVTecTestDataset(\n",
    "    root_path=MVTEC_DATASET_PATH,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a73855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RESUMEN DE DATASETS:\n",
      "Entrenamiento (Original): 2880 imágenes\n",
      "Validación (Original):    720 imágenes\n",
      "MVTec (Validación Visual): 124 imágenes\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n RESUMEN DE DATASETS:\")\n",
    "print(f\"Entrenamiento (Original): {len(train_dataset)} imágenes\")\n",
    "print(f\"Validación (Original):    {len(val_dataset)} imágenes\") \n",
    "print(f\"MVTec (Validación Visual): {len(mvtec_test_dataset)} imágenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331addab",
   "metadata": {},
   "source": [
    "#### Verificamos la distribución de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3eafeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DISTRIBUCIÓN DE CLASES:\n",
      "Entrenamiento (6 clases):\n",
      "  folding_marks: 480 imágenes\n",
      "  grain_off: 480 imágenes\n",
      "  growth_marks: 480 imágenes\n",
      "  loose_grain: 480 imágenes\n",
      "  non_defective: 480 imágenes\n",
      "  pinhole: 480 imágenes\n",
      "Validación (6 clases):\n",
      "  folding_marks: 120 imágenes\n",
      "  grain_off: 120 imágenes\n",
      "  growth_marks: 120 imágenes\n",
      "  loose_grain: 120 imágenes\n",
      "  non_defective: 120 imágenes\n",
      "  pinhole: 120 imágenes\n",
      "MVTec (validación visual):\n",
      "  Normal: 32 imágenes\n",
      "  Anomalía: 92 imágenes\n"
     ]
    }
   ],
   "source": [
    "# Verificar distribución de clases\n",
    "print(f\"\\n DISTRIBUCIÓN DE CLASES:\")\n",
    "if hasattr(train_dataset, 'labels'):\n",
    "    train_unique, train_counts = np.unique(train_dataset.labels, return_counts=True)\n",
    "    print(\"Entrenamiento (6 clases):\")\n",
    "    for class_id, count in zip(train_unique, train_counts):\n",
    "        print(f\"  {CLASS_NAMES[class_id]}: {count} imágenes\")\n",
    "\n",
    "if hasattr(val_dataset, 'labels'):\n",
    "    val_unique, val_counts = np.unique(val_dataset.labels, return_counts=True)\n",
    "    print(\"Validación (6 clases):\")\n",
    "    for class_id, count in zip(val_unique, val_counts):\n",
    "        print(f\"  {CLASS_NAMES[class_id]}: {count} imágenes\")\n",
    "\n",
    "if hasattr(mvtec_test_dataset, 'labels'):\n",
    "    mvtec_unique, mvtec_counts = np.unique(mvtec_test_dataset.labels, return_counts=True)\n",
    "    print(\"MVTec (validación visual):\")\n",
    "    mvtec_class_names = ['Normal', 'Anomalía']\n",
    "    for class_id, count in zip(mvtec_unique, mvtec_counts):\n",
    "        print(f\"  {mvtec_class_names[class_id]}: {count} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55dcd003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creando DataLoaders...\n",
      "24 workers para cargar los datasets\n",
      "- Train loader: 180 batches\n",
      "- Validation loader: 45 batches\n",
      "- MVTec test loader: 8 batches\n"
     ]
    }
   ],
   "source": [
    "# Crear DataLoaders\n",
    "print(\"\\n Creando DataLoaders...\")\n",
    "print(NUM_WORKERS, \"workers para cargar los datasets\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS) \n",
    "mvtec_test_loader = DataLoader(mvtec_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"- Train loader: {len(train_loader)} batches\")\n",
    "print(f\"- Validation loader: {len(val_loader)} batches\")\n",
    "print(f\"- MVTec test loader: {len(mvtec_test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea3c881a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c341d5fca60c44f1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c341d5fca60c44f1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Antes de entrenar lanzamos tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs --host 0.0.0.0 --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea225790",
   "metadata": {},
   "source": [
    "### Para acceder a tensorboard:\n",
    "\n",
    "http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b21b5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creando modelo ViT multi-clase...\n",
      "\n",
      " Iniciando entrenamiento con dataset Leather Defect...\n",
      " Entrenando en dataset (6 categorías)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.21it/s]\n",
      "Epoch 1 - Val: 100%|██████████| 45/45 [00:02<00:00, 16.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "  Train - Loss: 0.5425, Acc: 78.26%\n",
      "  Val   - Loss: 0.1893, Acc: 94.86%\n",
      "   Nuevo mejor modelo guardado! Acc: 94.86%\n",
      "  LR: 1.98e-05\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.21it/s]\n",
      "Epoch 2 - Val: 100%|██████████| 45/45 [00:02<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "  Train - Loss: 0.1843, Acc: 93.30%\n",
      "  Val   - Loss: 0.1482, Acc: 95.00%\n",
      "   Nuevo mejor modelo guardado! Acc: 95.00%\n",
      "  LR: 1.91e-05\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.19it/s]\n",
      "Epoch 3 - Val: 100%|██████████| 45/45 [00:02<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "  Train - Loss: 0.0948, Acc: 96.70%\n",
      "  Val   - Loss: 0.0863, Acc: 97.08%\n",
      "   Nuevo mejor modelo guardado! Acc: 97.08%\n",
      "  LR: 1.81e-05\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.41it/s]\n",
      "Epoch 4 - Val: 100%|██████████| 45/45 [00:02<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "  Train - Loss: 0.0714, Acc: 97.99%\n",
      "  Val   - Loss: 0.0825, Acc: 97.64%\n",
      "   Nuevo mejor modelo guardado! Acc: 97.64%\n",
      "  LR: 1.67e-05\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.43it/s]\n",
      "Epoch 5 - Val: 100%|██████████| 45/45 [00:02<00:00, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "  Train - Loss: 0.0874, Acc: 97.08%\n",
      "  Val   - Loss: 0.0919, Acc: 97.08%\n",
      "  LR: 1.50e-05\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.30it/s]\n",
      "Epoch 6 - Val: 100%|██████████| 45/45 [00:02<00:00, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "  Train - Loss: 0.0375, Acc: 98.68%\n",
      "  Val   - Loss: 0.0548, Acc: 98.61%\n",
      "   Nuevo mejor modelo guardado! Acc: 98.61%\n",
      "  LR: 1.31e-05\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.45it/s]\n",
      "Epoch 7 - Val: 100%|██████████| 45/45 [00:02<00:00, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "  Train - Loss: 0.0070, Acc: 99.86%\n",
      "  Val   - Loss: 0.0271, Acc: 99.17%\n",
      "   Nuevo mejor modelo guardado! Acc: 99.17%\n",
      "  LR: 1.10e-05\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.49it/s]\n",
      "Epoch 8 - Val: 100%|██████████| 45/45 [00:02<00:00, 16.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "  Train - Loss: 0.0008, Acc: 100.00%\n",
      "  Val   - Loss: 0.0251, Acc: 99.17%\n",
      "  LR: 8.95e-06\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.39it/s]\n",
      "Epoch 9 - Val: 100%|██████████| 45/45 [00:02<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "  Train - Loss: 0.0005, Acc: 100.00%\n",
      "  Val   - Loss: 0.0235, Acc: 99.31%\n",
      "   Nuevo mejor modelo guardado! Acc: 99.31%\n",
      "  LR: 6.91e-06\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.44it/s]\n",
      "Epoch 10 - Val: 100%|██████████| 45/45 [00:02<00:00, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "  Train - Loss: 0.0004, Acc: 100.00%\n",
      "  Val   - Loss: 0.0232, Acc: 99.31%\n",
      "  LR: 5.00e-06\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train: 100%|██████████| 180/180 [00:21<00:00,  8.27it/s]\n",
      "Epoch 11 - Val: 100%|██████████| 45/45 [00:02<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "  Train - Loss: 0.0004, Acc: 100.00%\n",
      "  Val   - Loss: 0.0227, Acc: 99.31%\n",
      "  LR: 3.31e-06\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train: 100%|██████████| 180/180 [00:20<00:00,  8.76it/s]\n",
      "Epoch 12 - Val: 100%|██████████| 45/45 [00:02<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15:\n",
      "  Train - Loss: 0.0003, Acc: 100.00%\n",
      "  Val   - Loss: 0.0227, Acc: 99.31%\n",
      "  LR: 1.91e-06\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train: 100%|██████████| 180/180 [00:20<00:00,  8.66it/s]\n",
      "Epoch 13 - Val: 100%|██████████| 45/45 [00:02<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15:\n",
      "  Train - Loss: 0.0003, Acc: 100.00%\n",
      "  Val   - Loss: 0.0227, Acc: 99.31%\n",
      "  LR: 8.65e-07\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train: 100%|██████████| 180/180 [00:20<00:00,  8.84it/s]\n",
      "Epoch 14 - Val: 100%|██████████| 45/45 [00:02<00:00, 17.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15:\n",
      "  Train - Loss: 0.0003, Acc: 100.00%\n",
      "  Val   - Loss: 0.0227, Acc: 99.31%\n",
      "  LR: 2.19e-07\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train: 100%|██████████| 180/180 [00:20<00:00,  8.91it/s]\n",
      "Epoch 15 - Val: 100%|██████████| 45/45 [00:02<00:00, 16.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15:\n",
      "  Train - Loss: 0.0003, Acc: 100.00%\n",
      "  Val   - Loss: 0.0227, Acc: 99.31%\n",
      "  LR: 0.00e+00\n",
      "------------------------------------------------------------\n",
      " Entrenamiento completado! Mejor accuracy: 99.31%\n"
     ]
    }
   ],
   "source": [
    "models_dir = 'models'  # Directorio para guardar el modelo entrenado\n",
    "model_name = 'best_modelo_kaggle_dataset'  # Nombre del modelo entrenado\n",
    "\n",
    "learning_rate = 2e-5  # Tasa de aprendizaje para el optimizador\n",
    "weight_decay = 1e-4  # Decaimiento de peso para regularización\n",
    "\n",
    "# Crear y entrenar modelo\n",
    "print(\"\\n Creando modelo ViT multi-clase...\")\n",
    "\n",
    "modelo_final = ViTMultiClassClassifier(\n",
    "    num_classes=len(CLASS_NAMES), \n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "print(\"\\n Iniciando entrenamiento con dataset Leather Defect...\")\n",
    "print(\" Entrenando en dataset (6 categorías)\")\n",
    "#model = train_multiclass_model(model, train_loader, val_loader, NUM_EPOCHS, device)\n",
    "model = train_model(modelo_final, train_loader, val_loader, learning_rate, weight_decay, NUM_EPOCHS, device, models_dir, logs_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cce5ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Extrayendo features normales para detección de anomalías...\n",
      " Usando clase 'non_defective' del dataset entrenado\n",
      "Extrayendo features de imágenes normales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando features normales: 100%|██████████| 180/180 [00:07<00:00, 23.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Almacenadas 480 features normales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraer features normales para detección de anomalías\n",
    "print(\"\\n Extrayendo features normales para detección de anomalías...\")\n",
    "print(\" Usando clase 'non_defective' del dataset entrenado\")\n",
    "modelo_final.store_normal_features(train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "264044ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EVALUACIÓN: Clasificación Multi-Clase (Dataset Leather Defect)\n",
      "============================================================\n",
      " Evaluación integral del modelo multi-clase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluación: 100%|██████████| 45/45 [00:03<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DISTRIBUCIÓN DE CLASES EN TEST:\n",
      "============================================================\n",
      "  folding_marks: 120 imágenes\n",
      "  grain_off: 120 imágenes\n",
      "  growth_marks: 120 imágenes\n",
      "  loose_grain: 120 imágenes\n",
      "  non_defective: 120 imágenes\n",
      "  pinhole: 120 imágenes\n",
      "\n",
      " RESULTADOS DE CLASIFICACIÓN MULTI-CLASE:\n",
      "============================================================\n",
      "Accuracy general: 0.9931\n",
      "\n",
      "Reporte detallado por clase:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "folding_marks     0.9917    0.9917    0.9917       120\n",
      "    grain_off     0.9916    0.9833    0.9874       120\n",
      " growth_marks     1.0000    1.0000    1.0000       120\n",
      "  loose_grain     1.0000    1.0000    1.0000       120\n",
      "non_defective     0.9836    1.0000    0.9917       120\n",
      "      pinhole     0.9916    0.9833    0.9874       120\n",
      "\n",
      "     accuracy                         0.9931       720\n",
      "    macro avg     0.9931    0.9931    0.9930       720\n",
      " weighted avg     0.9931    0.9931    0.9930       720\n",
      "\n",
      "\n",
      " RESULTADOS DE DETECCIÓN DE ANOMALÍAS:\n",
      "============================================================\n",
      "\n",
      "Hybrid (Paper Method):\n",
      "  ROC AUC:           1.0000\n",
      "  Average Precision: 1.0000\n",
      "  Binary Accuracy:   0.9986\n",
      "  Optimal Threshold: 0.2947\n",
      "\n",
      "Cosine Similarity:\n",
      "  ROC AUC:           1.0000\n",
      "  Average Precision: 1.0000\n",
      "  Binary Accuracy:   0.9972\n",
      "  Optimal Threshold: 0.3348\n",
      "\n",
      "Classification Confidence:\n",
      "  ROC AUC:           1.0000\n",
      "  Average Precision: 1.0000\n",
      "  Binary Accuracy:   0.9986\n",
      "  Optimal Threshold: 0.4313\n",
      "\n",
      " Generando visualizaciones...\n",
      " Creando ejemplos de clasificación multi-clase...\n",
      "  ✓ Ejemplos guardados: reports/best_modelo_kaggle_dataset/resultados_multiclase/ejemplos_multiclase.png\n"
     ]
    }
   ],
   "source": [
    "# Evaluación en dataset original (clasificación multi-clase)\n",
    "print(\"\\n EVALUACIÓN: Clasificación Multi-Clase (Dataset Leather Defect)\")\n",
    "print(\"=\" * 60)\n",
    "#def eval_model(model, test_loader, device, class_names, model_name, output_dir):\n",
    "original_results = eval_model(\n",
    "    modelo_final, val_loader, device, CLASS_NAMES, model_name, reports_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69399b53",
   "metadata": {},
   "source": [
    "#### Validación Cualitativa en MVTec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e2bd852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " VALIDACIÓN VISUAL: MVTec AD (Cualitativa)\n",
      "============================================================\n",
      " Generando validación visual ...\n",
      " Siguiendo el enfoque del paper: validación cualitativa únicamente\n",
      " Procesando batch 0...\n",
      " Generando attention maps para 16 imágenes...\n",
      " Procesando batch de 16 imágenes...\n",
      " Generados 16 attention maps\n",
      "  Mapa 0: min=0.121, max=0.306, std=0.015\n",
      "  Mapa 1: min=0.237, max=0.319, std=0.015\n",
      "  Mapa 2: min=0.317, max=0.426, std=0.019\n",
      "  Mapa 3: min=0.126, max=0.255, std=0.013\n",
      "  Mapa 4: min=0.290, max=0.432, std=0.025\n",
      "  Mapa 5: min=0.317, max=0.439, std=0.024\n",
      "  Mapa 6: min=0.159, max=0.301, std=0.015\n",
      "  Mapa 7: min=0.325, max=0.425, std=0.021\n",
      "  Mapa 8: min=0.126, max=0.271, std=0.013\n",
      "  Mapa 9: min=0.185, max=0.311, std=0.014\n",
      "  Mapa 10: min=0.188, max=0.298, std=0.017\n",
      "  Mapa 11: min=0.222, max=0.350, std=0.015\n",
      "  Mapa 12: min=0.288, max=0.383, std=0.018\n",
      "  Mapa 13: min=0.160, max=0.269, std=0.016\n",
      "  Mapa 14: min=0.282, max=0.397, std=0.022\n",
      "  Mapa 15: min=0.111, max=0.248, std=0.014\n",
      " Procesando 12 imágenes de ejemplo...\n",
      "    Validación visual guardada: reports/best_modelo_kaggle_dataset/resultados_mvtec_visual/validacion_visual_mvtec.png\n",
      "    Imágenes de referencia guardadas: reports/best_modelo_kaggle_dataset/resultados_mvtec_visual/imagenes_ejemplo_mvtec.png\n",
      "    Resumen detallado guardado: reports/best_modelo_kaggle_dataset/resultados_mvtec_visual/visual_validation_summary.txt\n",
      "\n",
      " VALIDACIÓN VISUAL COMPLETADA:\n",
      "   - 12 imágenes procesadas\n",
      "   - Mapas de calor generados según metodología del paper\n",
      "   - Sin métricas cuantitativas (siguiendo el paper)\n",
      "   - Enfoque en demostración visual de capacidades\n",
      "   - Procesamiento con manejo de errores\n"
     ]
    }
   ],
   "source": [
    "# Validación Visual en MVTec AD (como en el paper)\n",
    "print(\"\\n VALIDACIÓN VISUAL: MVTec AD (Cualitativa)\")\n",
    "print(\"=\" * 60)\n",
    "#mvtec_visual_results = visual_validation_mvtec(model, mvtec_test_loader, device, model_name, reports_path)\n",
    "\n",
    "mvtec_visual_results = visual_validation_mvtec(\n",
    "    modelo_final, mvtec_test_loader, device, model_name, reports_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ec2a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ¡EXPERIMENTO MULTI-CLASE COMPLETADO!\n",
      "============================================================\n",
      " ENTRENAMIENTO: Dataset original del paper (6 categorías)\n",
      "   Clasificación Multi-clase: 0.9931\n",
      "   Detección Anomalías (Original): 1.0000\n",
      "\n",
      " VALIDACIÓN VISUAL: MVTec AD (Enfoque del Paper)\n",
      "   Muestras procesadas: 12\n",
      "   Normales: 0\n",
      "   Anomalías: 12\n",
      "   Detecciones correctas: 12\n",
      "\n",
      " Resultados guardados en:\n",
      "  - results_original/ (clasificación multi-clase)\n",
      "  - results_mvtec_visual/ (validación visual MVTec)\n",
      "\n",
      " Método: Paper completo - Entrenamiento multi-clase + Validación visual MVTec\n",
      "\n",
      " Nota: MVTec usado solo para validación visual siguiendo metodología del paper\n",
      "     (sin métricas cuantitativas como recomienda el paper original)\n"
     ]
    }
   ],
   "source": [
    "# Resumen final\n",
    "print(\"\\n ¡EXPERIMENTO MULTI-CLASE COMPLETADO!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\" ENTRENAMIENTO: Dataset original del paper (6 categorías)\")\n",
    "print(f\"   Clasificación Multi-clase: {original_results['multiclass_accuracy']:.4f}\")\n",
    "\n",
    "if 'anomaly_detection_results' in original_results:\n",
    "    best_method = max(original_results['anomaly_detection_results'].keys(), \n",
    "                     key=lambda k: original_results['anomaly_detection_results'][k]['roc_auc'])\n",
    "    best_auc_original = original_results['anomaly_detection_results'][best_method]['roc_auc']\n",
    "    print(f\"   Detección Anomalías (Original): {best_auc_original:.4f}\")\n",
    "\n",
    "print(f\"\\n VALIDACIÓN VISUAL: MVTec AD (Enfoque del Paper)\")\n",
    "print(f\"   Muestras procesadas: {mvtec_visual_results['samples_processed']}\")\n",
    "print(f\"   Normales: {mvtec_visual_results['normal_samples']}\")\n",
    "print(f\"   Anomalías: {mvtec_visual_results['anomaly_samples']}\")\n",
    "print(f\"   Detecciones correctas: {mvtec_visual_results['correct_detections']}\")\n",
    "\n",
    "print(f\"\\n Resultados guardados en:\")\n",
    "print(f\"  - results_original/ (clasificación multi-clase)\")\n",
    "print(f\"  - results_mvtec_visual/ (validación visual MVTec)\")\n",
    "print(f\"\\n Método: Paper completo - Entrenamiento multi-clase + Validación visual MVTec\")\n",
    "print(f\"\\n Nota: MVTec usado solo para validación visual siguiendo metodología del paper\")\n",
    "print(f\"     (sin métricas cuantitativas como recomienda el paper original)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpc3-grupal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
