{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset"
      ],
      "metadata": {
        "id": "aNNnQIRvmn54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "import gdown\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc, average_precision_score, precision_recall_fscore_support, precision_recall_curve\n",
        "import sys\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-ATuREItmi06"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '1Xvmn1BylM1cPdl21xAO06Xqp9SpoXHiJ'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "data_path = \"data/\" # Defino el directorio de datos donde se guardará el archivo\n",
        "if not os.path.exists(data_path):\n",
        "    os.makedirs(data_path)  # Crea el directorio si no existe\n",
        "\n",
        "\n",
        "output_path = os.path.join(data_path, 'data.zip')\n",
        "\n",
        "gdown.download(url, output_path, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "1tmkaNMmmlOS",
        "outputId": "476e74d5-1ca0-4ec3-bc07-66d5203121e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Xvmn1BylM1cPdl21xAO06Xqp9SpoXHiJ\n",
            "From (redirected): https://drive.google.com/uc?id=1Xvmn1BylM1cPdl21xAO06Xqp9SpoXHiJ&confirm=t&uuid=2539b454-2131-47cf-9583-10003606cb2d\n",
            "To: /content/data/data.zip\n",
            "100%|██████████| 5.27G/5.27G [01:23<00:00, 63.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/data.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Descomprimiendo...\")\n",
        "\n",
        "# descomprimir el archivo ZIP\n",
        "with ZipFile(output_path, 'r') as zip_ref:\n",
        "    # Obtener la lista de archivos en el ZIP\n",
        "    file_list = zip_ref.namelist()\n",
        "\n",
        "    # Inicializar la barra de progreso\n",
        "    with tqdm(total=len(file_list), unit='file') as pbar:\n",
        "        for file in file_list:\n",
        "            zip_ref.extract(file, data_path)  # Extraer cada archivo\n",
        "            pbar.update(1)  # Actualizar la barra de progreso\n",
        "\n",
        "# Eliminar el archivo ZIP después de descomprimir\n",
        "os.remove(output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4ca7bf860bfd4cf995152e3688062ee4",
            "f5b92c0d00934a03bdf0e9bcfb7c070e",
            "090ec09961cd477c8b313587610d7406",
            "50017206e8854fe798674f6df1ea4569",
            "2fc8c81adb9947d894cf9522ebffcf40",
            "6f865f2592394c9baa46b72f8c73e2f2",
            "c22f159a8c614a76b78a8b0abc617f38",
            "5f1310fc6e8046da8cc9c83d046813e4",
            "8aa991454e7b41cf841dd18799b6468c",
            "dae44f0474d24a22afe4a853aab8c1d3",
            "397772acdbe74bb3b7987b0b5ffc7f17"
          ]
        },
        "id": "REy0NzP6mm3c",
        "outputId": "e3fca0fe-191c-48ec-a5f3-5fc046c4c214"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descomprimiendo...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6878 [00:00<?, ?file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ca7bf860bfd4cf995152e3688062ee4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "Sn-VSZormyR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ya esta en otro notebook"
      ],
      "metadata": {
        "id": "U0Co56mGmzp4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset y dataloader"
      ],
      "metadata": {
        "id": "zPBnjNL71mSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset y DataLoader para MVTec AD (sacada de 3.0.model-training.ipynb)\n",
        "class MVTecDataset(Dataset):\n",
        "    def __init__(self, root_path, category, is_train=True, transform=None, mask_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_path: Ruta al directorio raíz de MVTec AD\n",
        "            category: Categoría de objetos ('bottle', 'cable', 'carpet', etc.)\n",
        "            is_train: Si es True, carga imágenes de entrenamiento (normales)\n",
        "                      Si es False, carga imágenes de prueba (normales y anómalas)\n",
        "            transform: Transformaciones opcionales a aplicar a las imágenes\n",
        "            mask_transform: Transformaciones opcionales a aplicar a las máscaras\n",
        "        \"\"\"\n",
        "        self.root_path = root_path\n",
        "        self.category = category\n",
        "        self.is_train = is_train\n",
        "        self.transform = transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        # Definir directorios\n",
        "        if self.is_train:\n",
        "            self.image_dir = os.path.join(root_path, category, 'train', 'good')\n",
        "            self.image_paths = [os.path.join(self.image_dir, f) for f in os.listdir(self.image_dir)\n",
        "                               if f.endswith('.png')]\n",
        "            self.labels = np.zeros(len(self.image_paths), dtype=np.float32)  # 0 = normal\n",
        "            self.mask_paths = None\n",
        "\n",
        "        else:  # Test set\n",
        "            self.image_dir = os.path.join(root_path, category, 'test')\n",
        "            self.image_paths = []\n",
        "            self.labels = []\n",
        "            self.mask_paths = []\n",
        "\n",
        "            # Imágenes normales (buenas)\n",
        "            good_dir = os.path.join(self.image_dir, 'good')\n",
        "            if os.path.exists(good_dir):\n",
        "                good_images = [os.path.join(good_dir, f) for f in os.listdir(good_dir)\n",
        "                              if f.endswith('.png')]\n",
        "                self.image_paths.extend(good_images)\n",
        "                self.labels.extend([0] * len(good_images))  # 0 = normal\n",
        "                self.mask_paths.extend([None] * len(good_images))\n",
        "\n",
        "            # Imágenes anómalas (con defectos)\n",
        "            defect_types = [d for d in os.listdir(self.image_dir)\n",
        "                           if os.path.isdir(os.path.join(self.image_dir, d)) and d != 'good']\n",
        "\n",
        "            for defect in defect_types:\n",
        "                defect_dir = os.path.join(self.image_dir, defect)\n",
        "                defect_images = [os.path.join(defect_dir, f) for f in os.listdir(defect_dir)\n",
        "                                if f.endswith('.png')]\n",
        "                self.image_paths.extend(defect_images)\n",
        "                self.labels.extend([1] * len(defect_images))  # 1 = anomalía\n",
        "\n",
        "                # Añadir máscaras de ground truth (si existen)\n",
        "                gt_dir = os.path.join(root_path, category, 'ground_truth', defect)\n",
        "                if os.path.exists(gt_dir):\n",
        "                    for img_path in defect_images:\n",
        "                        img_name = os.path.basename(img_path)\n",
        "                        mask_name = img_name.replace('.png', '_mask.png')\n",
        "                        mask_path = os.path.join(gt_dir, mask_name)\n",
        "                        if os.path.exists(mask_path):\n",
        "                            self.mask_paths.append(mask_path)\n",
        "                        else:\n",
        "                            self.mask_paths.append(None)\n",
        "                else:\n",
        "                    self.mask_paths.extend([None] * len(defect_images))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Cargar imagen\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB') # Convertir a RGB\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Cargar máscara si existe (solo para test y anomalías)\n",
        "        mask = None\n",
        "        if not self.is_train and self.mask_paths[idx] is not None: # Si es test y hay máscara\n",
        "            mask_path = self.mask_paths[idx]\n",
        "            mask = Image.open(mask_path).convert('L') # Convertir a escala de grises\n",
        "            if self.mask_transform:\n",
        "                mask = self.mask_transform(mask) # Aplicar transformaciones a la máscara\n",
        "            elif self.transform:\n",
        "                mask = transforms.Compose([\n",
        "                    transforms.Resize((224, 224)),\n",
        "                    transforms.ToTensor(),\n",
        "                ])(mask) # Aplicar transformaciones por defecto a la máscara\n",
        "        else:\n",
        "            # Crear una máscara vacía si no existe\n",
        "            mask = torch.zeros((1, 224, 224))\n",
        "\n",
        "        # Aplicar transformaciones a la imagen\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Siempre devolver tres elementos\n",
        "        return image, label, mask"
      ],
      "metadata": {
        "id": "btbKsYwtv6nO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT MAE"
      ],
      "metadata": {
        "id": "fDpEI0Mg1o2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_vit_mae_anomaly_detection(data_path, category, reports_dir='reports_vit_mae/', num_epochs=10, batch_size_train=8, lr=2e-5):\n",
        "    \"\"\"\n",
        "    Fine-tunes un modelo ViT MAE para detección de anomalías en una categoría específica de MVTec AD.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "    print(f\"Usando dispositivo: {device}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # Crear subdirectorio para gráficas si no existe\n",
        "    plots_dir = os.path.join(reports_dir, \"plots\")\n",
        "    os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Cargar Procesador y Modelo\n",
        "    model_checkpoint = \"facebook/vit-mae-base\"\n",
        "    try:\n",
        "        image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
        "        model = ViTMAEForPreTraining.from_pretrained(model_checkpoint)\n",
        "        model.to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando modelo o procesador: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "    # 2. Preparar Datos\n",
        "    img_size = 224\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
        "    ])\n",
        "    test_transform = transforms.Compose([ # Sin augmentation para test\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
        "    ])\n",
        "\n",
        "    train_dataset = MVTecDataset(root_path=data_path, category=category, is_train=True, transform=train_transform)\n",
        "    if len(train_dataset) == 0:\n",
        "        print(f\"No se encontraron imágenes de entrenamiento para la categoría {category}. Omitiendo.\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "    test_dataset = MVTecDataset(root_path=data_path, category=category, is_train=False, transform=test_transform)\n",
        "    if len(test_dataset) == 0:\n",
        "        print(f\"No se encontraron imágenes de test para la categoría {category}. Omitiendo.\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    # Para evaluación, batch_size=1 para obtener la pérdida por instancia de ViTMAEForPreTraining\n",
        "    test_loader_per_instance = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # 3. Fine-tuning (Entrenamiento)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=lr/100)\n",
        "\n",
        "    train_losses_history = [] # Para almacenar la pérdida de cada época\n",
        "\n",
        "    print(f\"\\nIniciando fine-tuning para la categoría: {category}...\")\n",
        "    sys.stdout.flush()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for images, _, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"):\n",
        "            images = images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        train_losses_history.append(avg_epoch_loss)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Pérdida Media de Entrenamiento: {avg_epoch_loss:.4f}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Guardar gráfica de pérdida de entrenamiento\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses_history, marker='o', linestyle='-')\n",
        "    plt.title(f'Pérdida de Entrenamiento por Época - Categoría: {category}')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Pérdida Media de Entrenamiento')\n",
        "    plt.grid(True)\n",
        "    loss_plot_path = os.path.join(plots_dir, f'training_loss_{category}.png')\n",
        "    plt.savefig(loss_plot_path)\n",
        "    plt.close()\n",
        "    print(f\"Gráfica de pérdida de entrenamiento guardada en: {loss_plot_path}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # 4. Evaluación (Puntuación de Anomalía)\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_scores = []\n",
        "\n",
        "    print(f\"\\nEvaluando en el conjunto de test para la categoría: {category} (puntuación por instancia)...\")\n",
        "    sys.stdout.flush()\n",
        "    with torch.no_grad():\n",
        "        for images, labels_batch, _ in tqdm(test_loader_per_instance, desc=\"Evaluando Instancias\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            loss_val = outputs.loss.item()\n",
        "            all_scores.append(loss_val)\n",
        "            if isinstance(labels_batch, torch.Tensor):\n",
        "                all_labels.extend(labels_batch.cpu().numpy())\n",
        "            else:\n",
        "                all_labels.extend(np.array([labels_batch]))\n",
        "\n",
        "    if not all_labels or not all_scores:\n",
        "        print(f\"No se pudieron recolectar datos para la evaluación de {category}. Omitiendo AUC.\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "    all_labels_np = np.array(all_labels).astype(int)\n",
        "    if len(np.unique(all_labels_np)) < 2 : # Chequeo si hay al menos dos clases para AUC\n",
        "        print(f\"No hay suficientes clases únicas en las etiquetas para calcular AUC para {category} (Etiquetas: {np.unique(all_labels_np)}). Omitiendo AUC.\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "    # Calcular ROC AUC (Nivel de Imagen)\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(all_labels, all_scores)\n",
        "        print(f\"\\nResultados para la categoría: {category}\")\n",
        "        print(f\"ROC AUC a Nivel de Imagen: {roc_auc:.4f}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Guardar curva ROC\n",
        "        fpr, tpr, _ = roc_curve(all_labels_np, all_scores)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "        plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "        plt.title(f'Curva ROC - Categoría: {category}')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        roc_plot_path = os.path.join(plots_dir, f'roc_curve_{category}.png')\n",
        "        plt.savefig(roc_plot_path)\n",
        "        plt.close()\n",
        "        print(f\"Gráfica de curva ROC guardada en: {roc_plot_path}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"No se pudo calcular ROC AUC para {category}: {e}. Etiquetas únicas: {np.unique(all_labels_np)}\", file=sys.stderr)\n",
        "        return None # Retorna None si hay error en AUC\n",
        "\n",
        "    # Guardar el modelo fine-tuned\n",
        "    model_save_dir = os.path.join(reports_dir, \"trained_models\")\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "    model_save_path = os.path.join(model_save_dir, f\"vit_mae_finetuned_{category}.pth\")\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Modelo fine-tuned guardado en {model_save_path}\")\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "\n",
        "# --- Ejecución Principal ---\n",
        "if __name__ == \"__main__\":\n",
        "    MVTEC_AD_PATH = 'data/'\n",
        "\n",
        "    if not os.path.exists(MVTEC_AD_PATH) or not os.listdir(MVTEC_AD_PATH):\n",
        "        print(f\"Error: El directorio del dataset MVTec AD '{MVTEC_AD_PATH}' no existe o está vacío.\", file=sys.stderr)\n",
        "        print(\"Por favor, descarga el dataset MVTec AD y actualiza la variable MVTEC_AD_PATH.\", file=sys.stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Categorías a procesar\n",
        "    categories_to_run = [\"bottle\", \"cable\"] # Un subconjunto para una prueba rápida\n",
        "\n",
        "    all_category_results = {}\n",
        "    num_epochs_main = 15\n",
        "    learning_rate_main = 1e-5\n",
        "    batch_size_training_main = 16\n",
        "\n",
        "    for cat_name in categories_to_run:\n",
        "        print(f\"\\n{'='*20} PROCESANDO CATEGORÍA: {cat_name.upper()} {'='*20}\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Verificar si el directorio de la categoría existe\n",
        "        category_dir_check = os.path.join(MVTEC_AD_PATH, cat_name)\n",
        "        if not os.path.exists(category_dir_check):\n",
        "            print(f\"Directorio para la categoría '{cat_name}' no encontrado en '{category_dir_check}'. Omitiendo.\", file=sys.stderr)\n",
        "            continue\n",
        "\n",
        "        auc = run_vit_mae_anomaly_detection(\n",
        "            data_path=MVTEC_AD_PATH,\n",
        "            category=cat_name,\n",
        "            num_epochs=num_epochs_main,\n",
        "            batch_size_train=batch_size_training_main,\n",
        "            lr=learning_rate_main\n",
        "        )\n",
        "        if auc is not None:\n",
        "            all_category_results[cat_name] = auc\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "    print(\"\\n\\n{'='*30} RESUMEN FINAL DE ROC AUC POR CATEGORÍA {'='*30}\")\n",
        "    if all_category_results:\n",
        "        for cat_name, auc_score in all_category_results.items():\n",
        "            print(f\"Categoría: {cat_name:<15} | ROC AUC: {auc_score:.4f}\")\n",
        "\n",
        "        valid_auc_scores = [score for score in all_category_results.values() if score is not None]\n",
        "        if valid_auc_scores:\n",
        "            avg_auc = np.mean(valid_auc_scores)\n",
        "            print(f\"\\nROC AUC Promedio (sobre categorías exitosas): {avg_auc:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo se pudieron calcular puntuaciones AUC válidas.\")\n",
        "    else:\n",
        "        print(\"No se generaron resultados para ninguna categoría.\")\n",
        "    sys.stdout.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T06XBRiYsLn3",
        "outputId": "336e2d95-e49e-4156-d8ec-869a72a1e96c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== PROCESANDO CATEGORÍA: BOTTLE ====================\n",
            "Usando dispositivo: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando fine-tuning para la categoría: bottle...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/15 (Train): 100%|██████████| 14/14 [00:07<00:00,  1.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Pérdida Media de Entrenamiento: 0.0159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2/15 (Train): 100%|██████████| 14/14 [00:08<00:00,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15, Pérdida Media de Entrenamiento: 0.0157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3/15 (Train): 100%|██████████| 14/14 [00:07<00:00,  1.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15, Pérdida Media de Entrenamiento: 0.0145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4/15 (Train): 100%|██████████| 14/14 [00:08<00:00,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15, Pérdida Media de Entrenamiento: 0.0143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5/15 (Train): 100%|██████████| 14/14 [00:08<00:00,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15, Pérdida Media de Entrenamiento: 0.0140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6/15 (Train): 100%|██████████| 14/14 [00:07<00:00,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15, Pérdida Media de Entrenamiento: 0.0139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7/15 (Train): 100%|██████████| 14/14 [00:08<00:00,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/15, Pérdida Media de Entrenamiento: 0.0144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8/15 (Train): 100%|██████████| 14/14 [00:07<00:00,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/15, Pérdida Media de Entrenamiento: 0.0138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9/15 (Train): 100%|██████████| 14/14 [00:07<00:00,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/15, Pérdida Media de Entrenamiento: 0.0132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10/15 (Train): 100%|██████████| 14/14 [00:08<00:00,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/15, Pérdida Media de Entrenamiento: 0.0137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11/15 (Train): 100%|██████████| 14/14 [00:07<00:00,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15, Pérdida Media de Entrenamiento: 0.0128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12/15 (Train): 100%|██████████| 14/14 [00:08<00:00,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/15, Pérdida Media de Entrenamiento: 0.0131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13/15 (Train): 100%|██████████| 14/14 [00:08<00:00,  1.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/15, Pérdida Media de Entrenamiento: 0.0132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 14/15 (Train): 100%|██████████| 14/14 [00:07<00:00,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/15, Pérdida Media de Entrenamiento: 0.0128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 15/15 (Train): 100%|██████████| 14/14 [00:08<00:00,  1.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/15, Pérdida Media de Entrenamiento: 0.0132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gráfica de pérdida de entrenamiento guardada en: reports_vit_mae/plots/training_loss_bottle.png\n",
            "\n",
            "Evaluando en el conjunto de test para la categoría: bottle (puntuación por instancia)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluando Instancias: 100%|██████████| 83/83 [00:03<00:00, 25.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resultados para la categoría: bottle\n",
            "ROC AUC a Nivel de Imagen: 0.7746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gráfica de curva ROC guardada en: reports_vit_mae/plots/roc_curve_bottle.png\n",
            "Modelo fine-tuned guardado en reports_vit_mae/trained_models/vit_mae_finetuned_bottle.pth\n",
            "\n",
            "==================== PROCESANDO CATEGORÍA: CABLE ====================\n",
            "Usando dispositivo: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando fine-tuning para la categoría: cable...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/15 (Train): 100%|██████████| 14/14 [00:11<00:00,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Pérdida Media de Entrenamiento: 0.1469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15, Pérdida Media de Entrenamiento: 0.1402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15, Pérdida Media de Entrenamiento: 0.1348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15, Pérdida Media de Entrenamiento: 0.1358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15, Pérdida Media de Entrenamiento: 0.1342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15, Pérdida Media de Entrenamiento: 0.1321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/15, Pérdida Media de Entrenamiento: 0.1315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/15, Pérdida Media de Entrenamiento: 0.1332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/15, Pérdida Media de Entrenamiento: 0.1301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/15, Pérdida Media de Entrenamiento: 0.1279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15, Pérdida Media de Entrenamiento: 0.1334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/15, Pérdida Media de Entrenamiento: 0.1296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13/15 (Train): 100%|██████████| 14/14 [00:12<00:00,  1.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/15, Pérdida Media de Entrenamiento: 0.1278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 14/15 (Train): 100%|██████████| 14/14 [00:13<00:00,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/15, Pérdida Media de Entrenamiento: 0.1318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 15/15 (Train): 100%|██████████| 14/14 [00:11<00:00,  1.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/15, Pérdida Media de Entrenamiento: 0.1245\n",
            "Gráfica de pérdida de entrenamiento guardada en: reports_vit_mae/plots/training_loss_cable.png\n",
            "\n",
            "Evaluando en el conjunto de test para la categoría: cable (puntuación por instancia)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluando Instancias: 100%|██████████| 150/150 [00:07<00:00, 18.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resultados para la categoría: cable\n",
            "ROC AUC a Nivel de Imagen: 0.6064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gráfica de curva ROC guardada en: reports_vit_mae/plots/roc_curve_cable.png\n",
            "Modelo fine-tuned guardado en reports_vit_mae/trained_models/vit_mae_finetuned_cable.pth\n",
            "\n",
            "\n",
            "{'='*30} RESUMEN FINAL DE ROC AUC POR CATEGORÍA {'='*30}\n",
            "Categoría: bottle          | ROC AUC: 0.7746\n",
            "Categoría: cable           | ROC AUC: 0.6064\n",
            "\n",
            "ROC AUC Promedio (sobre categorías exitosas): 0.6905\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ca7bf860bfd4cf995152e3688062ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5b92c0d00934a03bdf0e9bcfb7c070e",
              "IPY_MODEL_090ec09961cd477c8b313587610d7406",
              "IPY_MODEL_50017206e8854fe798674f6df1ea4569"
            ],
            "layout": "IPY_MODEL_2fc8c81adb9947d894cf9522ebffcf40"
          }
        },
        "f5b92c0d00934a03bdf0e9bcfb7c070e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f865f2592394c9baa46b72f8c73e2f2",
            "placeholder": "​",
            "style": "IPY_MODEL_c22f159a8c614a76b78a8b0abc617f38",
            "value": "100%"
          }
        },
        "090ec09961cd477c8b313587610d7406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f1310fc6e8046da8cc9c83d046813e4",
            "max": 6878,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aa991454e7b41cf841dd18799b6468c",
            "value": 6878
          }
        },
        "50017206e8854fe798674f6df1ea4569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dae44f0474d24a22afe4a853aab8c1d3",
            "placeholder": "​",
            "style": "IPY_MODEL_397772acdbe74bb3b7987b0b5ffc7f17",
            "value": " 6878/6878 [01:00&lt;00:00, 145.32file/s]"
          }
        },
        "2fc8c81adb9947d894cf9522ebffcf40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f865f2592394c9baa46b72f8c73e2f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c22f159a8c614a76b78a8b0abc617f38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f1310fc6e8046da8cc9c83d046813e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa991454e7b41cf841dd18799b6468c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dae44f0474d24a22afe4a853aab8c1d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "397772acdbe74bb3b7987b0b5ffc7f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}